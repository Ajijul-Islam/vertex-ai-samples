{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1jg2qBjVb4yf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAeljAi7b4yg"
      },
      "source": [
        "# Vertex AI Model Garden - Fine-tuning with Axolotl\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_axolotl_finetuning.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_axolotl_finetuning.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5NXBxyjf1xs"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates fine-tuning using [Axolotl](https://github.com/axolotl-ai-cloud/axolotl). Axolotl streamlines AI model fine-tuning by providing a wide range of training recipes and supporting multiple configurations and architectures.\n",
        "\n",
        "We can use either Enterprise Colab runtime or Vertex AI training for fine-tuning using axolotl.\n",
        "Colab runtime has below advantages:\n",
        "- **Sanity check for flags**: Use Enterprise Colab runtime to do sanity check for Axolotl flags before running it on Vertex AI training directly.\n",
        "- **Quick experimentations**: Use Enterprise Colab runtime to do quick experimentations with Axolotl flags. The [max-steps](https://github.com/axolotl-ai-cloud/axolotl/blob/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/docs/config.qmd#L360) flag is useful to limit the training time.\n",
        "- **Debugging**: Use Enterprise Colab runtime to debug axolotl fine-tuning. This can be more efficient because debugging on the Vertex AI training involves waiting for resources to be provisioned, which can add delays. Also it is easier to add debug statements on Enterprise Colab runtime compared to Vertex AI training.\n",
        "\n",
        "Once the local fine-tuning is verified, the Vertex AI training is the recommended way to run the fine-tuning. Vertex AI training has several advantages, including:\n",
        "- **Running multiple training jobs in parallel**: This can be useful for hyperparameter tuning or running experiments with different datasets etc.\n",
        "- **For High End GPU**: Vertex AI training provides access to higher-end GPUs like the H100, which can be crucial if you encounter out-of-memory (OOM) errors.\n",
        "- **[DWS support](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws)**: DWS makes Vertex AI training more cost-effective, and easier to manage, especially in scenarios where GPU availability is a concern.\n",
        "Refer to [this documentation](https://cloud.google.com/vertex-ai/docs/training/overview#vertexi-ai-operationalizes-training-at-scale) for more details on Vertex AI training advantages.\n",
        "\n",
        "### Objective\n",
        "- Train model using Axolotl in local Enterprise Colab runtime.\n",
        "- Train model using Axolotl with Vertex AI Training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8JgAE4BQTj"
      },
      "source": [
        "## [Optional] Setup Colab Runtime\n",
        "**You need to setup the Colab Runtime with L4 GPU or A100 GPU if you want to run local finetuning. The following sections perform the setup for L4 GPU.**\n",
        "To learn more about creating runtime, you can optionally read [this](https://cloud.google.com/colab/docs/create-runtime).\n",
        "\n",
        "**Note: make sure to create a runtime with appropriate machine type and gpu type to avoid out of memory issues. [Refer this](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to decide which machine type and gpu type to select.**\n",
        "\n",
        "**Note: We recommend using a runtime environment configured with NVIDIA_TESLA_A100 with 4 GPUs or any other multi-GPU machine with higher GPU memory.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ggq03Jf5nNFp"
      },
      "outputs": [],
      "source": [
        "# @title Create runtime\n",
        "# @markdown This cell creates a runtime template and then creates a runtime using that template.\n",
        "# @markdown **If you have already created a runtime, you can skip this cell.**\n",
        "# @markdown This cell can take up to 5 minutes to run.\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "RUNTIME_PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "RUNTIME_REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "RUNTIME_ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\"  # @param [\"\", \"NVIDIA_L4\", \"NVIDIA_TESLA_A100\", \"NVIDIA_A100_80GB\"]\n",
        "RUNTIME_ACCELERATOR_COUNT = \"4\"  # @param [1, 2, 4, 8, 16]\n",
        "RUNTIME_ACCELERATOR_COUNT = int(RUNTIME_ACCELERATOR_COUNT)\n",
        "\n",
        "if not RUNTIME_ACCELERATOR_TYPE:\n",
        "  print(\"Warning: No accelerator type specified. Skipping runtime creation.\")\n",
        "  try:\n",
        "    subprocess.check_output(\"nvidia-smi\")\n",
        "    print(\"Nvidia GPU detected!\")\n",
        "  except Exception:\n",
        "    print(\"Warning: Nvidia GPU not detected. Use GPU runtime for local fine-tuning.\")\n",
        "else:\n",
        "  if RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_L4\" and RUNTIME_ACCELERATOR_COUNT == 1:\n",
        "    RUNTIME_MACHINE_TYPE = \"g2-standard-8\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_L4\" and RUNTIME_ACCELERATOR_COUNT == 2:\n",
        "    RUNTIME_MACHINE_TYPE = \"g2-standard-24\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_L4\" and RUNTIME_ACCELERATOR_COUNT == 4:\n",
        "    RUNTIME_MACHINE_TYPE = \"g2-standard-48\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_L4\" and RUNTIME_ACCELERATOR_COUNT == 8:\n",
        "    RUNTIME_MACHINE_TYPE = \"g2-standard-96\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\" and RUNTIME_ACCELERATOR_COUNT != 16:\n",
        "    RUNTIME_MACHINE_TYPE = f\"a2-highgpu-{RUNTIME_ACCELERATOR_COUNT}g\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\" and RUNTIME_ACCELERATOR_COUNT == 16:\n",
        "    RUNTIME_MACHINE_TYPE = \"a2-megagpu-16g\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_A100_80GB\" and RUNTIME_ACCELERATOR_COUNT in [1, 2, 4, 8]:\n",
        "    RUNTIME_MACHINE_TYPE = f\"a2-ultragpu-{RUNTIME_ACCELERATOR_COUNT}g\"\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid GPU type {RUNTIME_ACCELERATOR_TYPE}, and count {RUNTIME_ACCELERATOR_COUNT} combination.\")\n",
        "\n",
        "  print(f\"Machine type: {RUNTIME_MACHINE_TYPE}\")\n",
        "\n",
        "  uuid = uuid.uuid4()\n",
        "  RUNTIME_DISPLAY_NAME = f\"axolotl-{RUNTIME_ACCELERATOR_TYPE}-{RUNTIME_ACCELERATOR_COUNT}-{uuid}\"\n",
        "  print(f\"Creating runtime with display name: {RUNTIME_DISPLAY_NAME}\")\n",
        "\n",
        "  # create runtime template\n",
        "  shell_output = ! gcloud colab runtime-templates create --display-name=$RUNTIME_DISPLAY_NAME \\\n",
        "    --project=$RUNTIME_PROJECT_ID --region=$RUNTIME_REGION \\\n",
        "    --machine-type=$RUNTIME_MACHINE_TYPE --accelerator-type=$RUNTIME_ACCELERATOR_TYPE \\\n",
        "    --accelerator-count=$RUNTIME_ACCELERATOR_COUNT --disk-type=PD_BALANCED\n",
        "  shell_output = \"\\n\".join(shell_output)\n",
        "  print(shell_output)\n",
        "  RUNTIME_TEMPLATE_ID = re.search(r\"projects/.*/locations/.*/notebookRuntimeTemplates/(\\d+)\", shell_output).group(1)\n",
        "\n",
        "  # create runtime\n",
        "  shell_output = ! gcloud colab runtimes create --display-name=$RUNTIME_DISPLAY_NAME \\\n",
        "    --runtime-template=$RUNTIME_TEMPLATE_ID --project=$RUNTIME_PROJECT_ID \\\n",
        "    --region=$RUNTIME_REGION\n",
        "  shell_output = \"\\n\".join(shell_output)\n",
        "  print(shell_output)\n",
        "  RUNTIME_ID = re.search(r\"projects/.*/locations/.*/notebookRuntimes/(\\d+)\", shell_output).group(1)\n",
        "\n",
        "  # start runtime\n",
        "  ! gcloud colab runtimes start $RUNTIME_ID --project=$RUNTIME_PROJECT_ID --region=$RUNTIME_REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGDpB0NnnNFp"
      },
      "source": [
        "### Connect to runtime manually\n",
        "If you are planning to do local finetuning, you have to **connect manually to the runtime by following [the instructions here](https://cloud.google.com/colab/docs/connect-to-runtime).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kik-beBAnNFq"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XeJdEWe9nNFq"
      },
      "outputs": [],
      "source": [
        "# @title [Optional for Vertex AI fine-tuning] Setup Pytorch for local fine-tuning\n",
        "# @markdown **Note: This section must be run before performing local fine-tuning.** This section installs correct pytorch dependency needed for the Axolotl local run.\n",
        "import os\n",
        "\n",
        "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"7.0 7.5 8.0 8.6 9.0+PTX\"\n",
        "! pip install torch==2.4.1 torchvision\n",
        "os.environ[\"PYTORCH_INSTALLATION\"] = \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_cAAzli5nNFq"
      },
      "outputs": [],
      "source": [
        "# @title Import utility packages for fine-tuning\n",
        "\n",
        "# Import the necessary packages.\n",
        "! rm -rf vertex-ai-samples && git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "! cd vertex-ai-samples\n",
        "\n",
        "# Import the necessary packages.\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "import yaml\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "train_job = None\n",
        "models, endpoints = {}, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k32BrMnWnNFq"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning, follow [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws) to use Dynamic Workload Scheduler. For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs, and [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_gpus) quota for Nvidia Tesla A100 GPUs. To train using L4 gpus with default quota, check [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_l4_gpus) quota for Nvidia L4 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"axolotl\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7AfA9UsnNFq"
      },
      "source": [
        "## Finetune with Axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-oKISrDMnNFr"
      },
      "outputs": [],
      "source": [
        "# @title Set Axolotl config\n",
        "\n",
        "# @markdown You can use below axolotl configs taken from [examples directory](https://github.com/axolotl-ai-cloud/axolotl/tree/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/examples), which have been verified by model garden team through internal testing. Note that we have used A100 80GB and H100 80GB GPU for testing.\n",
        "# @markdown > | Model Name | Base Model | Axolotl Config |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | code-llama | codellama/CodeLlama-7b-hf | examples/code-llama/7b/lora.yml |\n",
        "# @markdown | code-llama | codellama/CodeLlama-7b-hf | examples/code-llama/7b/qlora.yml |\n",
        "# @markdown | code-llama | codellama/CodeLlama-13b-hf | examples/code-llama/13b/lora.yml |\n",
        "# @markdown | code-llama | codellama/CodeLlama-13b-hf | examples/code-llama/13b/qlora.yml |\n",
        "# @markdown | code-llama | codellama/CodeLlama-34b-hf | examples/code-llama/34b/lora.yml |\n",
        "# @markdown | code-llama | codellama/CodeLlama-34b-hf | examples/code-llama/34b/qlora.yml |\n",
        "# @markdown | falcon | tiiuae/falcon-7b | examples/falcon/config-7b-lora.yml |\n",
        "# @markdown | falcon | tiiuae/falcon-7b | examples/falcon/config-7b.yml |\n",
        "# @markdown | gemma | google/gemma-7b | examples/gemma/qlora.yml |\n",
        "# @markdown | llama-2 | NousResearch/Llama-2-7b-hf | examples/llama-2/fft_optimized.yml |\n",
        "# @markdown | llama-2 | NousResearch/Llama-2-7b-hf | examples/llama-2/loftq.yml |\n",
        "# @markdown | llama-2 | NousResearch/Llama-2-7b-hf | examples/llama-2/lora.yml |\n",
        "# @markdown | llama-2 | NousResearch/Llama-2-7b-hf | examples/llama-2/qlora-fsdp.yml |\n",
        "# @markdown | llama-2 | NousResearch/Llama-2-7b-hf | examples/llama-2/qlora.yml |\n",
        "# @markdown | llama-3 | NousResearch/Meta-Llama-3.1-8B | examples/llama-3/fft-8b.yaml |\n",
        "# @markdown | llama-3 | NousResearch/Meta-Llama-3-8B-Instruct | examples/llama-3/instruct-lora-8b.yml |\n",
        "# @markdown | llama-3 | NousResearch/Meta-Llama-3-8B | examples/llama-3/lora-8b.yml |\n",
        "# @markdown | llama-3 | NousResearch/Llama-3.2-1B | examples/llama-3/qlora-1b.yml |\n",
        "# @markdown | llama-3 | casperhansen/llama-3-70b-fp16 | examples/llama-3/qlora-fsdp-70b.yaml |\n",
        "# @markdown | mistral | mistralai/Mistral-7B-v0.1 | examples/mistral/config.yml |\n",
        "# @markdown | mistral | mistralai/Mistral-7B-v0.1 | examples/mistral/lora-mps.yml |\n",
        "# @markdown | mistral | mistralai/Mistral-7B-v0.1 | examples/mistral/lora.yml |\n",
        "# @markdown | mistral | mistralai/Mistral-7B-v0.1 | examples/mistral/mistral-qlora-orpo.yml |\n",
        "# @markdown | mistral | mistral-community/Mixtral-8x22B-v0.1 | examples/mistral/mixtral-8x22b-qlora-fsdp.yml |\n",
        "# @markdown | mistral | mistralai/Mixtral-8x7B-v0.1 | examples/mistral/mixtral-qlora-fsdp.yml |\n",
        "# @markdown | mistral | mistralai/Mistral-7B-v0.1 | examples/mistral/qlora.yml |\n",
        "# @markdown | openllama-3b | openlm-research/open_llama_3b_v2 | examples/openllama-3b/config.yml |\n",
        "# @markdown | openllama-3b | openlm-research/open_llama_3b_v2 | examples/openllama-3b/lora.yml |\n",
        "# @markdown | openllama-3b | openlm-research/open_llama_3b_v2 | examples/openllama-3b/qlora.yml |\n",
        "# @markdown | phi | microsoft/Phi-3.5-mini-instruct | examples/phi/lora-3.5.yaml |\n",
        "# @markdown | phi | microsoft/phi-1_5 | examples/phi/phi-ft.yml |\n",
        "# @markdown | phi | microsoft/phi-1_5 | examples/phi/phi-qlora.yml |\n",
        "# @markdown | phi | microsoft/phi-2 | examples/phi/phi2-ft.yml |\n",
        "# @markdown | phi | microsoft/Phi-3-mini-4k-instruct | examples/phi/phi3-ft.yml |\n",
        "# @markdown | qwen | Qwen/Qwen1.5-MoE-A2.7B | examples/qwen/qwen2-moe-lora.yaml |\n",
        "# @markdown | qwen | Qwen/Qwen1.5-MoE-A2.7B | examples/qwen/qwen2-moe-qlora.yaml |\n",
        "# @markdown | qwen2 | Qwen/Qwen2.5-0.5B | examples/qwen2/dpo.yaml |\n",
        "# @markdown | qwen2 | Qwen/Qwen2-7B | examples/qwen2/qlora-fsdp.yaml |\n",
        "# @markdown | qwen3 | Qwen/Qwen3-8B | examples/qwen3/qlora-fsdp.yaml |\n",
        "# @markdown | qwen3 | Qwen/Qwen3-32B | examples/qwen3/32b-qlora.yaml |\n",
        "# @markdown | tiny-llama | TinyLlama/TinyLlama_v1.1 | examples/tiny-llama/lora-mps.yml |\n",
        "# @markdown | tiny-llama | TinyLlama/TinyLlama_v1.1 | examples/tiny-llama/lora.yml |\n",
        "# @markdown | tiny-llama | TinyLlama/TinyLlama-1.1B-Chat-v1.0 | examples/tiny-llama/pretrain.yml |\n",
        "# @markdown | tiny-llama | TinyLlama/TinyLlama_v1.1 | examples/tiny-llama/qlora.yml |\n",
        "\n",
        "# @markdown 1. Set Axolotl config source.<br>\n",
        "# @markdown For `GITHUB` as source, you can explore different Axolotl configurations in the [examples directory](https://github.com/axolotl-ai-cloud/axolotl/tree/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/examples). For `GITHUB` source, `AXOLOTL_CONFIG_PATH` should start with `examples/`. e.g. examples/tiny-llama/lora.yml.<br>\n",
        "# @markdown For `LOCAL` as source, create Axolotl config yaml file and specify correct path below. Note that, the local file will be copied to GCS bucket before running Vertex AI training job. For `LOCAL` source, `AXOLOTL_CONFIG_PATH` should be a complete path of the config file. e.g. /content/lora.yml.<br>\n",
        "# @markdown For `GCS` as source, specify the GCS URI to the Axolotl config file. Make sure the file is accessible to service account used in the notebook. For `GCS` source, `AXOLOTL_CONFIG_PATH` should be a complete GCS URI of the config file. e.g. gs://bucket/path/to/config/file.yml.\n",
        "\n",
        "AXOLOTL_SOURCE = \"GITHUB\"  # @param [\"GITHUB\", \"LOCAL\", \"GCS\"]\n",
        "\n",
        "# @markdown 2. Set the Axolotl config file path.\n",
        "AXOLOTL_CONFIG_PATH = \"examples/tiny-llama/lora.yml\"  # @param {type:\"string\"}\n",
        "\n",
        "assert AXOLOTL_CONFIG_PATH, \"AXOLOTL_CONFIG_PATH must be set.\"\n",
        "\n",
        "if AXOLOTL_SOURCE == \"GITHUB\":\n",
        "    assert AXOLOTL_CONFIG_PATH.startswith(\n",
        "        \"examples/\"\n",
        "    ), \"AXOLOTL_CONFIG_PATH must start with examples/ for GITHUB source.\"\n",
        "    github_url = f\"https://github.com/axolotl-ai-cloud/axolotl/raw/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/{AXOLOTL_CONFIG_PATH}\"\n",
        "    r = requests.get(github_url)\n",
        "    axolotl_config = r.content.decode(\"utf-8\")\n",
        "    axolotl_config = yaml.safe_load(axolotl_config)\n",
        "elif AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    config_path = pathlib.Path(AXOLOTL_CONFIG_PATH)\n",
        "    assert config_path.exists(), \"AXOLOTL_CONFIG_PATH must exist for LOCAL source.\"\n",
        "    file_content = config_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "elif AXOLOTL_SOURCE == \"GCS\":\n",
        "    local_path = pathlib.Path(\"/content/tmp/axolotl_config.yml\")\n",
        "    common_util.download_gcs_file_to_local(AXOLOTL_CONFIG_PATH, local_path.absolute())\n",
        "    file_content = local_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "    AXOLOTL_CONFIG_PATH = common_util.gcs_fuse_path(AXOLOTL_CONFIG_PATH)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported AXOLOTL_SOURCE: {AXOLOTL_SOURCE}\")\n",
        "\n",
        "OUTPUT_GCS_URI = MODEL_BUCKET\n",
        "\n",
        "if not OUTPUT_GCS_URI.startswith(\"gs://\"):\n",
        "    OUTPUT_GCS_URI = f\"gs://{OUTPUT_GCS_URI}\"\n",
        "\n",
        "output_sub_dir = (\n",
        "    AXOLOTL_CONFIG_PATH.replace(\"/\", \"_\").replace(\".yaml\", \"\").replace(\".yml\", \"\")\n",
        ")\n",
        "BASE_AXOLOTL_OUTPUT_GCS_URI = f\"{OUTPUT_GCS_URI}/{output_sub_dir}/axolotl_output\"\n",
        "BASE_AXOLOTL_OUTPUT_DIR = common_util.gcs_fuse_path(BASE_AXOLOTL_OUTPUT_GCS_URI)\n",
        "\n",
        "# Placeholders for dataset settings.\n",
        "datasets = []\n",
        "test_datasets = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_iTQ4nSlODZJ"
      },
      "outputs": [],
      "source": [
        "# @title **[Optional]** Setup HF token\n",
        "# @markdown Some models like Gemma2, Mistral, Llama3 etc require a token to access with [gated access from huggingface](https://huggingface.co/docs/hub/en/models-gated).\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4bp13RSoODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Setup dataset\n",
        "\n",
        "# @markdown This section configures the dataset used for fine-tuning. **Note: If you don't fill any of the dataset options given below, then the dataset used will be the one defined in the Axolotl config file.** You have two options to configure the dataset:\n",
        "\n",
        "# @markdown **1. Use a Hugging Face Dataset**\n",
        "# @markdown   - Requires specifying the dataset name and type.\n",
        "\n",
        "# @markdown **2. Load from Google Cloud Storage (GCS)**\n",
        "# @markdown   - Requires specifying the bucket name, dataset type, file type, and paths to training/test splits.\n",
        "\n",
        "# @markdown **Choose ONE of the following options:**\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 1: Hugging Face**\n",
        "\n",
        "# @markdown **Hugging Face Dataset Name:**\n",
        "HF_DATASET = \"\"  # @param {type:\"string\", placeholder: \"e.g. timdettmers/openassistant-guanaco\"}\n",
        "# @markdown **Set the dataset type:** Refer to [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/docs/config.qmd#L87) for more details.\n",
        "HF_DATASET_TYPE = \"\"  # @param {type:\"string\", placeholder: \"e.g. completion\"}\n",
        "if HF_DATASET:\n",
        "    assert HF_DATASET_TYPE, \"HF_DATASET_TYPE must be set if HF_DATASET is set.\"\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 2: GCS**\n",
        "\n",
        "# @markdown **Bucket Name:**\n",
        "DATASET_BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **Dataset Type:** Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/docs/config.qmd#L181) for more details.\n",
        "DATASET_TYPE = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **File Type**. Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/docs/config.qmd#L178).\n",
        "FILE_TYPE = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown **Path to Training Data (relative to bucket):**\n",
        "TRAIN_DATAFILES_PATH = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **[Optional] Path to Test Data (relative to bucket):**\n",
        "# @markdown To use a dedicated validation set, provide the file path. Otherwise, the training data will be split to create a validation set.\n",
        "TEST_DATAFILES_PATH = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if DATASET_BUCKET_NAME:\n",
        "    assert (\n",
        "        TRAIN_DATAFILES_PATH\n",
        "    ), \"TRAIN_DATAFILES_PATH must be set if DATASET_BUCKET_NAME is set.\"\n",
        "    assert DATASET_TYPE, \"DATASET_TYPE must be set if DATASET_BUCKET_NAME is set.\"\n",
        "    assert FILE_TYPE, \"FILE_TYPE must be set if DATASET_BUCKET_NAME is set.\"\n",
        "\n",
        "assert not (\n",
        "    HF_DATASET and DATASET_BUCKET_NAME\n",
        "), \"Only one of HF_DATASET or DATASET_BUCKET_NAME can be set.\"\n",
        "\n",
        "if DATASET_BUCKET_NAME:\n",
        "    paths = TRAIN_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    datasets.append(dataset)\n",
        "\n",
        "if TEST_DATAFILES_PATH:\n",
        "    paths = TEST_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    test_datasets.append(dataset)\n",
        "\n",
        "if HF_DATASET:\n",
        "    datasets.append({\"path\": HF_DATASET, \"type\": HF_DATASET_TYPE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3SOjc9q8ODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Setup Axolotl Flags\n",
        "# @markdown This section configures additional Axolotl flags. You can explore different Axolotl flags in the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/c7d07de6b47b1b11d2098589e4bb15c6ed1066c3/docs/config.qmd).\n",
        "\n",
        "# @markdown **To avoid OOM, you can reduce sequence length.** This can be done by setting `sequence_len` flag to some smaller value. But reducing sequence length will also reduce the model performance.\n",
        "# @markdown **Another alternative to avoid OOM is to use higher memory gpu.** It is recommended to use vertex ai training for Higher memory gpu like A100 and H100. Vertex AI training offers greater availability of high-end GPUs.\n",
        "\n",
        "# @markdown **Training can take a long time (20+ hours) to complete depending on the model, dataset and axololt config.** You can reduce the training time by reducing the max training steps. This can be done by setting `max_steps` flag to some smaller value. Note that this will also reduce the model performance.\n",
        "\n",
        "# @markdown If you want to override base model then you can use `base_model` flag.\n",
        "\n",
        "# @markdown For example, let's say you want to log results to tensorboard and also want to use Qwen/Qwen3-32B model, then you can set [\"--use-tensorboard=True\", \"--base_model=Qwen/Qwen3-32B\"] in below `axolotl_flag_overrides` to achieve that.\n",
        "\n",
        "axolotl_flag_overrides = [\"--use-tensorboard=True\"]  # @param {type:\"raw\"}\n",
        "assert type(axolotl_flag_overrides) is list, \"axolotl_flag_overrides must be a list.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0M8kqcMbwamp"
      },
      "outputs": [],
      "source": [
        "# @title Set model id and publisher\n",
        "\n",
        "base_model = axolotl_config[\"base_model\"]\n",
        "for overrides in axolotl_flag_overrides:\n",
        "    if overrides.startswith(\"--base_model=\"):\n",
        "        base_model = overrides.split(\"=\")[1]\n",
        "        break\n",
        "publisher = base_model.split(\"/\")[0]\n",
        "model_id = base_model.split(\"/\")[1]\n",
        "model_id = model_id.replace(\".\", \"-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrZ8eQJKODZJ"
      },
      "source": [
        "### Finetune with Local Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aQ0v6wsyODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Install Axolotl And GCSFUSE\n",
        "# @markdown Check machine type\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    subprocess.check_output(\"nvidia-smi\")\n",
        "    print(\"Nvidia GPU detected!\")\n",
        "except Exception:\n",
        "    raise ValueError(\"Nvidia GPU not detected. Use GPU runtime for local fine-tuning.\")\n",
        "\n",
        "# @markdown Check if correct pytorch is installed.\n",
        "if \"PYTORCH_INSTALLATION\" not in os.environ:\n",
        "    raise ValueError(\n",
        "        \"pytorch is not installed. Install it from `Setup Pytorch for local fine-tuning` section of the notebook.\"\n",
        "    )\n",
        "\n",
        "# @markdown Install axolotl\n",
        "! rm -rf axolotl\n",
        "! git clone https://github.com/axolotl-ai-cloud/axolotl.git\n",
        "! cd axolotl && git reset --hard c7d07de6b47b1b11d2098589e4bb15c6ed1066c3\n",
        "! pip3 install packaging ninja\n",
        "! cd axolotl && pip3 install --no-build-isolation -e '.[flash-attn,deepspeed,llmcompressor,ring-flash-attn,optimizers]'\n",
        "! cd axolotl && python scripts/unsloth_install.py | sh\n",
        "! cd axolotl && python scripts/cutcrossentropy_install.py | sh\n",
        "\n",
        "# @markdown Install GCSFUSE\n",
        "! apt-get install gcsfuse -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GREqaOn1ODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Run Local fine-tuning\n",
        "# @markdown This section runs the Axolotl training locally (i.e. colab runtime).\n",
        "# @markdown **Note: This section can take a long time to run. You can reduce the training time by reducing the max training steps as mentioned in `Setup Axolotl Flags` section.**\n",
        "# @markdown Model trained using Axolotl will be saved in the GCS bucket with the help of GCSFUSE.\n",
        "\n",
        "# @markdown 1. Run GCSFUSE so that axolotl can store the training output in the GCS bucket.\n",
        "! mkdir -p /gcs/\n",
        "! gcsfuse /gcs\n",
        "\n",
        "# @markdown 2. Set up huggingface cache dir.\n",
        "os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "\n",
        "# @markdown 3. Run Axolotl training.\n",
        "\n",
        "finetuning_time = time.time_ns()\n",
        "AXOLOTL_OUTPUT_GCS_URI = (\n",
        "    f\"{BASE_AXOLOTL_OUTPUT_GCS_URI}/local/time_ns_{finetuning_time}\"\n",
        ")\n",
        "AXOLOTL_OUTPUT_DIR = f\"{BASE_AXOLOTL_OUTPUT_DIR}/local/time_ns_{finetuning_time}\"\n",
        "axolotl_args = f\" --output-dir={AXOLOTL_OUTPUT_DIR}\"\n",
        "if len(datasets) > 0:\n",
        "    axolotl_args += f' --datasets=\"{datasets}\"'\n",
        "if len(test_datasets) > 0:\n",
        "    axolotl_args += f' --test-datasets=\"{test_datasets}\"'\n",
        "    axolotl_args += \" --val-set-size=0\"\n",
        "additional_flags = \" \".join(axolotl_flag_overrides)\n",
        "axolotl_args += f\" {additional_flags}\"\n",
        "! accelerate launch -m axolotl.cli.train $axolotl_args /content/axolotl/$AXOLOTL_CONFIG_PATH\n",
        "\n",
        "# @markdown 4. Check the output in the bucket.\n",
        "! gsutil ls $AXOLOTL_OUTPUT_GCS_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eD_66aBjODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Run Local inference\n",
        "# @markdown This section performs inference using the finetuned model.\n",
        "\n",
        "# @markdown 1. Run Axolotl inference using gradio on local finetuned model.\n",
        "! cd axolotl && export CUDA_VISIBLE_DEVICES=0 && axolotl inference  examples/tiny-llama/lora.yml --output-dir=$AXOLOTL_OUTPUT_DIR --gradio\n",
        "\n",
        "# @markdown 2. After running the cell, a public URL ([\"https://*.gradio.live\"](#)) will appear in the cell output. The playground is available in a separate browser tab when you click the URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VEGLvAlnODZK"
      },
      "outputs": [],
      "source": [
        "# @title Create merged model\n",
        "# @markdown This section merges the finetuned adapter with the base model.\n",
        "# @markdown **Note: This is only needed for lora and qlora. In case of full finetuning you can skip this cell.**\n",
        "\n",
        "if (\n",
        "    \"adapter\" in axolotl_config\n",
        "    and axolotl_config[\"adapter\"] != \"lora\"\n",
        "    and axolotl_config[\"adapter\"] != \"qlora\"\n",
        "):\n",
        "    raise ValueError(\"This cell is only needed for lora and qlora.\")\n",
        "\n",
        "# @markdown 1. Run Axolotl merge.\n",
        "! cd axolotl && python3 -m axolotl.cli.merge_lora $axolotl_args $AXOLOTL_CONFIG_PATH --output-dir=$AXOLOTL_OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7V8fEjODZK"
      },
      "source": [
        "### Finetune with Vertex AI Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0XqH_Mvelhon"
      },
      "outputs": [],
      "source": [
        "# @title Vertex AI fine-tuning job\n",
        "# @markdown This section runs the Axolotl training using Vertex AI training job.\n",
        "# @markdown **Note: This section can take a long time to run. You can reduce the training time by reducing the max training steps as mentioned in `Setup Axolotl Flags` section.**\n",
        "# @markdown Refer to [Axolotl config](https://axolotl-ai-cloud.github.io/axolotl/docs/config.html) to override additional Axolotl flags.\n",
        "\n",
        "from google.cloud.aiplatform.compat.types import \\\n",
        "    custom_job as gca_custom_job_compat\n",
        "\n",
        "# @markdown Acceletor type to use for training.\n",
        "training_accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_L4\", \"NVIDIA_TESLA_A100\", \"NVIDIA_H100_80GB\", \"NVIDIA_A100_80GB\"]\n",
        "\n",
        "\n",
        "replica_count = 1\n",
        "repo = \"us-docker.pkg.dev/vertex-ai\"\n",
        "per_node_accelerator_count = 1\n",
        "boot_disk_size_gb = 500\n",
        "dws_kwargs = {\n",
        "    \"max_wait_duration\": 1800,  # 30 minutes\n",
        "    \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "}\n",
        "is_dynamic_workload_scheduler = True\n",
        "if training_accelerator_type == \"NVIDIA_L4\":\n",
        "    training_machine_type = \"g2-standard-8\"\n",
        "    is_dynamic_workload_scheduler = False\n",
        "    dws_kwargs = {}\n",
        "elif training_accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "    training_machine_type = \"a2-highgpu-1g\"\n",
        "elif training_accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    per_node_accelerator_count = 8\n",
        "    training_machine_type = \"a2-ultragpu-8g\"\n",
        "elif training_accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    training_machine_type = \"a3-highgpu-8g\"\n",
        "    per_node_accelerator_count = 8\n",
        "    boot_disk_size_gb = 2000\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported accelerator type: {training_accelerator_type}\")\n",
        "\n",
        "TRAIN_DOCKER_URI = (\n",
        "    f\"{repo}/vertex-vision-model-garden-dockers/axolotl-train-dws:stable_20250429\"\n",
        ")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count * replica_count,\n",
        "    is_for_training=True,\n",
        "    is_restricted_image=False,\n",
        "    is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
        ")\n",
        "\n",
        "# @markdown Run Vertex AI job.\n",
        "\n",
        "# Copy the config file to the bucket.\n",
        "if AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    ! gsutil -m cp $AXOLOTL_CONFIG_PATH $MODEL_BUCKET/config/\n",
        "    AXOLOTL_CONFIG_PATH = f\"{common_util.gcs_fuse_path(MODEL_BUCKET)}/config/{pathlib.Path(AXOLOTL_CONFIG_PATH).name}\"\n",
        "\n",
        "# Set axolotl flags.\n",
        "datasets = []\n",
        "if DATASET_BUCKET_NAME:\n",
        "    paths = TRAIN_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    datasets.append(dataset)\n",
        "\n",
        "test_datasets = []\n",
        "if TEST_DATAFILES_PATH:\n",
        "    paths = TEST_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    test_datasets.append(dataset)\n",
        "\n",
        "if HF_DATASET:\n",
        "    datasets.append({\"path\": HF_DATASET, \"type\": HF_DATASET_TYPE})\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "AXOLOTL_OUTPUT_GCS_URI = f\"{BASE_AXOLOTL_OUTPUT_GCS_URI}/{job_name}\"\n",
        "AXOLOTL_OUTPUT_DIR = f\"{BASE_AXOLOTL_OUTPUT_DIR}/{job_name}\"\n",
        "\n",
        "TRAINING_JOB_OUTPUT_DIR = f\"{AXOLOTL_OUTPUT_GCS_URI}/training_job_output\"\n",
        "\n",
        "axolotl_config_overwrites = []\n",
        "axolotl_config_overwrites.append(f\"--output_dir={AXOLOTL_OUTPUT_DIR}\")\n",
        "if len(datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f'--datasets=\"{datasets}\"')\n",
        "if len(test_datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f'--test_datasets=\"{test_datasets}\"')\n",
        "    axolotl_config_overwrites.append(\"--val_set_size=0\")\n",
        "axolotl_config_overwrites += axolotl_flag_overrides\n",
        "\n",
        "train_job_args = []\n",
        "train_job_args.append(f\"--axolotl_config_path={AXOLOTL_CONFIG_PATH}\")\n",
        "train_job_args += axolotl_config_overwrites\n",
        "\n",
        "\n",
        "train_job_envs = {}\n",
        "if HF_TOKEN:\n",
        "    train_job_envs[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_axolotl_finetuning.ipynb\".split(\".\")[0],\n",
        "}\n",
        "\n",
        "model_name = AXOLOTL_CONFIG_PATH.split(\"/\")[1]\n",
        "labels[\"mg-tune\"] = f\"publishers-{publisher}-models-{model_name}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{model_id}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = labels[\"versioned-mg-tune\"][\n",
        "    : min(len(labels[\"versioned-mg-tune\"]), 63)\n",
        "]\n",
        "\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "print(\"Running training job with args:\")\n",
        "print(\" \\\\\\n\".join(train_job_args))\n",
        "train_job.run(\n",
        "    args=train_job_args,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=training_machine_type,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    boot_disk_size_gb=boot_disk_size_gb,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    base_output_dir=TRAINING_JOB_OUTPUT_DIR,\n",
        "    sync=False,  # Non-blocking call to run.\n",
        "    **dws_kwargs,\n",
        ")\n",
        "\n",
        "# Wait until resource has been created.\n",
        "train_job.wait_for_resource_creation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BI63V2IIha"
      },
      "source": [
        "### Run TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RcSJinPWXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section shows how to launch TensorBoard in a [Cloud Shell](https://cloud.google.com/shell/docs).\n",
        "# @markdown 1. Click the Cloud Shell icon(![terminal](https://github.com/google/material-design-icons/blob/master/png/action/terminal/materialicons/24dp/1x/baseline_terminal_black_24dp.png?raw=true)) on the top right to open the Cloud Shell.\n",
        "# @markdown 2. Copy the `tensorboard` command shown below by running this cell.\n",
        "# @markdown 3. Paste and run the command in the Cloud Shell to launch TensorBoard.\n",
        "# @markdown 4. Once the command runs (You may have to click `Authorize` if prompted), click the link starting with `http://localhost`.\n",
        "\n",
        "# @markdown Note: You may need to wait around 10 minutes after the job starts in order for the TensorBoard logs to be written to the GCS bucket.\n",
        "print(f\"Command to copy: tensorboard --logdir {AXOLOTL_OUTPUT_GCS_URI}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2rc_AAUwoLf"
      },
      "source": [
        "## Deploy using SGLang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YtvBYekJcke4"
      },
      "outputs": [],
      "source": [
        "# @markdown **Note:** This section is currently only for the Qwen3 models. For any other models, use the `Deploy using vLLM` section.\n",
        "# @markdown 1. Wait for the training job to finish.\n",
        "if train_job and train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "# @markdown 2. Set up SGLang docker URI and model gcs uri.\n",
        "\n",
        "if \"qwen3\" not in model_id.lower():\n",
        "    raise ValueError(\n",
        "        \"This section is only for the Qwen3 models. \"\n",
        "        \"For any other models, use the 'Deploy using vLLM' section\"\n",
        "    )\n",
        "\n",
        "SGLANG_MODEL_GCS_URI = AXOLOTL_OUTPUT_GCS_URI\n",
        "\n",
        "if \"adapter\" in axolotl_config and (\n",
        "    axolotl_config[\"adapter\"] == \"lora\" or axolotl_config[\"adapter\"] == \"qlora\"\n",
        "):\n",
        "    SGLANG_MODEL_GCS_URI = f\"{AXOLOTL_OUTPUT_GCS_URI}/merged\"\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "SGLANG_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:20250428-1803-rc0\"\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_H100_80GB\", \"NVIDIA_L4\"] {isTemplate:true}\n",
        "\n",
        "if accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    resource_id = \"custom_model_serving_nvidia_h100_gpus\"\n",
        "    machine_type = \"a3-highgpu-2g\"\n",
        "    accelerator_count = 2\n",
        "elif accelerator_type == \"NVIDIA_L4\":\n",
        "    resource_id = \"custom_model_serving_nvidia_l4_gpus\"\n",
        "    if model_id in [\"Qwen3-32B\"]:\n",
        "        machine_type = \"g2-standard-48\"\n",
        "        accelerator_count = 4\n",
        "    elif model_id in [\n",
        "        \"Qwen3-8B\",\n",
        "    ]:\n",
        "        machine_type = \"g2-standard-12\"\n",
        "        accelerator_count = 1\n",
        "    else:\n",
        "        raise ValueError(f\"Recommended GPU setting not found for: {model_id}.\")\n",
        "else:\n",
        "    raise ValueError(f\"Recommended GPU setting not found for: {model_id}.\")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU2Qskckmish"
      },
      "source": [
        "### Create model endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IEhCySY1mbI1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "from google import auth\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_sglang_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = \"\",\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float | None = None,\n",
        "    context_length: int | None = None,\n",
        "    dtype: str | None = None,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_torch_compile: bool = False,\n",
        "    enable_flashinfer_mla: bool = False,\n",
        "    disable_cuda_graph: bool = False,\n",
        "    speculative_algorithm: str | None = None,\n",
        "    speculative_draft_model_path: str = \"\",\n",
        "    speculative_num_steps: int = 3,\n",
        "    speculative_eagle_topk: int = 1,\n",
        "    speculative_num_draft_tokens: int = 4,\n",
        "    enable_jit_deepgemm: bool = False,\n",
        "    enable_dp_attention: bool = False,\n",
        "    dp_size: int = 1,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int | None = None,\n",
        "    is_spot: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with SGLang into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.sglang.ai/backend/server_arguments.html for a list of possible arguments with descriptions.\n",
        "    sglang_args = [\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tp={accelerator_count * multihost_gpu_node_count}\",\n",
        "        f\"--dp={dp_size}\",\n",
        "    ]\n",
        "\n",
        "    if context_length:\n",
        "        sglang_args.append(f\"--context-length={context_length}\")\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        sglang_args.append(f\"--mem-fraction-static={gpu_memory_utilization}\")\n",
        "\n",
        "    if max_num_seqs:\n",
        "        sglang_args.append(f\"--max-running-requests={max_num_seqs}\")\n",
        "\n",
        "    if dtype:\n",
        "        sglang_args.append(f\"--dtype={dtype}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        sglang_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enable_torch_compile:\n",
        "        sglang_args.append(\"--enable-torch-compile\")\n",
        "\n",
        "    if enable_flashinfer_mla:\n",
        "        sglang_args.append(\"--enable-flashinfer-mla\")\n",
        "\n",
        "    if disable_cuda_graph:\n",
        "        sglang_args.append(\"--disable-cuda-graph\")\n",
        "\n",
        "    if speculative_algorithm:\n",
        "        sglang_args.append(f\"--speculative-algorithm={speculative_algorithm}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-draft-model-path={speculative_draft_model_path}\"\n",
        "        )\n",
        "        sglang_args.append(f\"--speculative-num-steps={speculative_num_steps}\")\n",
        "        sglang_args.append(f\"--speculative-eagle-topk={speculative_eagle_topk}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-num-draft-tokens={speculative_num_draft_tokens}\"\n",
        "        )\n",
        "\n",
        "    if enable_dp_attention:\n",
        "        sglang_args.append(\"--enable-dp-attention\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    if enable_jit_deepgemm:\n",
        "        env_vars[\"SGL_ENABLE_JIT_DEEPGEMM\"] = \"1\"\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SGLANG_DOCKER_URI,\n",
        "        serving_container_args=sglang_args,\n",
        "        serving_container_ports=[30000],\n",
        "        serving_container_predict_route=\"/vertex_generate\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_axolotl_finetuning.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "LABEL = \"sglang_gpu\"\n",
        "models[LABEL], endpoints[LABEL] = deploy_model_sglang_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"axolotl-sglang-serve\"),\n",
        "    model_id=SGLANG_MODEL_GCS_URI,\n",
        "    publisher=publisher.lower(),\n",
        "    publisher_model_id=model_id.lower(),\n",
        "    base_model_id=model_id.lower(),\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    is_spot=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_AWmj1anByt"
      },
      "source": [
        "### Perform Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "04RSb1a_nDVv"
      },
      "outputs": [],
      "source": [
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by SGLang can be found [here](https://docs.sglang.ai/backend/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, the user is asking about the best way to diagnose and fix a flickering light in their house. Let me start by breaking down the steps. First, I need to consider the possible causes of flickering. Common issues could be a faulty light bulb, a blown fuse, a short circuit, or a problem with the circuit itself.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# @markdown A chat template formatted prompt for Qwen3 models are shown below as an example.\n",
        "prompt = \"<|im_start|>user What is the best way to diagnose and fix a flickering light in my house?<|im_end|><|im_start|>assistant\"  # @param {type: \"string\"}\n",
        "\n",
        "# @markdown By default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses.\n",
        "# @markdown The model will generate think content wrapped in a \\...\\ block, followed by the final response.\n",
        "# @markdown `max_new_tokens` may need to be increased to accommodate the additional think content.\n",
        "enable_thinking = True  # @param {type:\"boolean\"}\n",
        "if not enable_thinking:\n",
        "    prompt += \"\"\n",
        "\n",
        "\n",
        "max_new_tokens = 1024  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Sampling parameters such as `temperature` and `top_p` are automatically set according to [Qwen3's best practices](https://huggingface.co/Qwen/Qwen3-30B-A3B#best-practices).\n",
        "if enable_thinking:\n",
        "    temperature = 0.6\n",
        "    top_p = 0.95\n",
        "    top_k = 20\n",
        "    min_p = 0\n",
        "else:\n",
        "    temperature = 0.7\n",
        "    top_p = 0.8\n",
        "    top_k = 20\n",
        "    min_p = 0\n",
        "\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_new_tokens`.\n",
        "\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"sampling_params\": {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"min_p\": min_p,\n",
        "    }\n",
        "}\n",
        "response = endpoints[\"sglang_gpu\"].predict(\n",
        "    instances=instances,\n",
        "    parameters=parameters,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1BYNnfXy9_"
      },
      "source": [
        "## Deploy using VLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Up326e7kXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown 1. Wait for the training job to finish.\n",
        "if train_job and train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "# @markdown 2. Set up VLLM docker URI and model gcs uri.\n",
        "\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250405_1205_RC01\"\n",
        "VLLM_MODEL_GCS_URI = AXOLOTL_OUTPUT_GCS_URI\n",
        "\n",
        "if \"adapter\" in axolotl_config and (\n",
        "    axolotl_config[\"adapter\"] == \"lora\" or axolotl_config[\"adapter\"] == \"qlora\"\n",
        "):\n",
        "    VLLM_MODEL_GCS_URI = f\"{AXOLOTL_OUTPUT_GCS_URI}/merged\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsUk-xkXy9_"
      },
      "source": [
        "### Create model endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KSW-gb6nXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown 1. Set the machine type and accelerator type.\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
        "machine_type = \"g2-standard-12\"  # @param {type:\"string\"}\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param {type:\"string\"}\n",
        "per_node_accelerator_count = 1  # @param {type:\"integer\"}\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 2048\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        "    enable_llama_tool_parser: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    if enable_llama_tool_parser:\n",
        "        vllm_args.append(\"--enable-auto-tool-choice\")\n",
        "        vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_axolotl_finetuning.ipynb\",\n",
        "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"axolotl-vllm-serve\"),\n",
        "    publisher=publisher.lower(),\n",
        "    publisher_model_id=model_id.lower(),\n",
        "    model_id=VLLM_MODEL_GCS_URI,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    enable_lora=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO76I0p3Xy9_"
      },
      "source": [
        "### Perform Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kC9Apto7Xy9_"
      },
      "outputs": [],
      "source": [
        "def predict_vllm(\n",
        "    prompt: str,\n",
        "    max_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    raw_response: bool,\n",
        "    lora_weight: str = \"\",\n",
        "):\n",
        "    # Parameters for inference.\n",
        "    instance = {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    }\n",
        "    if lora_weight:\n",
        "        instance[\"dynamic-lora\"] = lora_weight\n",
        "    instances = [instance]\n",
        "    response = endpoints[\"vllm_gpu\"].predict(\n",
        "        instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        "    )\n",
        "\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoints[\"vllm_gpu\"] = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"Write a function to list n Fibonacci numbers in Python.\"  # @param {type: \"string\"}\n",
        "max_tokens = 500  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = True  # @param {type:\"boolean\"}\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "# \"<|file_separator|>\" is the end of the file token.\n",
        "for prediction in response.predictions:\n",
        "    print(prediction.split(\"<|file_separator|>\")[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2grDDphYx4zI"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_Pqw3TsF2uG4"
      },
      "outputs": [],
      "source": [
        "# @markdown Delete the training job.\n",
        "\n",
        "if train_job:\n",
        "    train_job.delete()\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_axolotl_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
