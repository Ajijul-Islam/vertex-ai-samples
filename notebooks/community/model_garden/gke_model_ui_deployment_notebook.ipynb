{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pr9TgOcV9vAXeqGiyTaTI5kS",
      "metadata": {
        "cellView": "form",
        "id": "Pr9TgOcV9vAXeqGiyTaTI5kS"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M1CpgYundFwz",
      "metadata": {
        "id": "M1CpgYundFwz"
      },
      "source": [
        "# Get started with your deployed model on GKE\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fgke_model_ui_deployment_notebook.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/gke_model_ui_deployment_notebook.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t2jj2XOgkS4F",
      "metadata": {
        "id": "t2jj2XOgkS4F"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook will guide you through the initial step of testing your recently deployed model with text prompts. Depending on your deployed model's inference setup, the notebook utilizes either Text Generation Inference [TGI](https://huggingface.co/docs/text-generation-inference/en/index) or [vLLM](https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/#:~:text=model%20frameworks%20simple.-,What%20is%20vLLM%3F,-vLLM%20is%20an), two efficient serving frameworks that enhance the performance of your GPU model. Ready to see your deployed model respond? Run the cells below and start experimenting with different prompts!\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before proceeding with this notebook, ensure you have already deployed a model using the Google Cloud Console. You can find an overview of AI and Machine Learning services on [GKE AI/ML](https://console.cloud.google.com/kubernetes/aiml/overview).\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Enable prompt-based testing of the AI model deployed on GKE\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes, such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "### Understanding the Inference Frameworks\n",
        "\n",
        "Your model is running on one of two popular and efficient serving frameworks: vLLM or Text Generation Inference (TGI). The following sections provide a brief overview of each to give you context on the underlying technology powering your model.\n",
        "\n",
        "\n",
        "#### TGI\n",
        "\n",
        "TGI is a highly optimized open-source LLM serving framework that can increase serving throughput on GPUs. TGI includes features such as:\n",
        "\n",
        "* Optimized transformer implementation with PagedAttention\n",
        "* Continuous batching to improve the overall serving throughput\n",
        "* Tensor parallelism and distributed serving on multiple GPUs\n",
        "\n",
        "To learn more, refer to the [TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md)\n",
        "\n",
        "#### vLLM\n",
        "\n",
        "vLLM is another fast and easy-to-use library for LLM inference and serving. It's known for its high throughput and efficiency, and it leverages PagedAttention. Key features include:\n",
        "\n",
        "* PagedAttention: Efficient memory management for handling long sequences and dynamic workloads.\n",
        "* Continuous batching: Maximizes GPU utilization by batching incoming requests.\n",
        "* High-throughput serving: Designed for production-level serving with low latency.\n",
        "* Optimized CUDA kernels.\n",
        "\n",
        "To learn more, refer to the [vLLM documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMf-T58TkDy1",
      "metadata": {
        "cellView": "form",
        "id": "XMf-T58TkDy1"
      },
      "outputs": [],
      "source": [
        "# @title # Connect to Google Cloud Project\n",
        "# @markdown #### Run this cell to configure your Google Cloud environment for Kubernetes (GKE) operations.\n",
        "\n",
        "# @markdown #### Actions:\n",
        "# @markdown 1.  **Connects to Project & Region:** Retrieves and sets your Google Cloud project ID and region.\n",
        "# @markdown 3.  **Installs `kubectl`:** Installs the Kubernetes command-line tool.\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1oG8ymQenHyD",
      "metadata": {
        "cellView": "form",
        "id": "1oG8ymQenHyD"
      },
      "outputs": [],
      "source": [
        "# @title # Select Cluster and Deployment { vertical-output: true }\n",
        "\n",
        "# @markdown ## Instruction:\n",
        "\n",
        "# @markdown This cell provides interactive dropdown menus to select a Google Kubernetes Engine (GKE) cluster and a deployment within that cluster.\n",
        "\n",
        "# @markdown ***Please select a cluster and deployment before proceeding.***\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "SELECTED_DEPLOYMENT = None\n",
        "\n",
        "\n",
        "def get_clusters(p, r):\n",
        "    try:\n",
        "        return (\n",
        "            subprocess.run(\n",
        "                [\n",
        "                    \"gcloud\",\n",
        "                    \"container\",\n",
        "                    \"clusters\",\n",
        "                    \"list\",\n",
        "                    \"--project\",\n",
        "                    p,\n",
        "                    \"--region\",\n",
        "                    r,\n",
        "                    \"--format=value(name)\",\n",
        "                ],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True,\n",
        "            )\n",
        "            .stdout.strip()\n",
        "            .split(\"\\n\")\n",
        "        )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_deployments(c, r):\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\n",
        "                \"gcloud\",\n",
        "                \"container\",\n",
        "                \"clusters\",\n",
        "                \"get-credentials\",\n",
        "                c,\n",
        "                \"--location\",\n",
        "                r,\n",
        "            ],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "        deployments = json.loads(\n",
        "            subprocess.run(\n",
        "                [\"kubectl\", \"get\", \"deployments\", \"-o\", \"json\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True,\n",
        "            ).stdout\n",
        "        )\n",
        "        return [i[\"metadata\"][\"name\"] for i in deployments[\"items\"]]\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def create_deployment_dropdown(cluster_name, region, on_select_deployment):\n",
        "    deployments = get_deployments(cluster_name, region)\n",
        "    deployments_with_prompt = [\"Select Deployment\"] + deployments\n",
        "    deployment_dropdown = widgets.Dropdown(\n",
        "        options=deployments_with_prompt,\n",
        "        description=\"Deployments\",\n",
        "        disabled=False,\n",
        "        width=\"4000px\",\n",
        "    )\n",
        "    deployment_dropdown.observe(\n",
        "        lambda c: on_select_deployment(c[\"new\"])\n",
        "        if c[\"type\"] == \"change\" and c[\"name\"] == \"value\"\n",
        "        else None,\n",
        "        names=\"value\",\n",
        "    )\n",
        "    return deployment_dropdown\n",
        "\n",
        "\n",
        "def on_deployment_select(deployment_name):\n",
        "    global SELECTED_DEPLOYMENT\n",
        "    SELECTED_DEPLOYMENT = deployment_name\n",
        "    print(f\"Selected deployment: {SELECTED_DEPLOYMENT}\")\n",
        "\n",
        "\n",
        "def on_cluster_change(change):\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        if change[\"new\"] == \"Select Cluster\":\n",
        "            return\n",
        "        deployment_dropdown = create_deployment_dropdown(\n",
        "            change[\"new\"], REGION, on_deployment_select\n",
        "        )\n",
        "        display(deployment_dropdown)\n",
        "\n",
        "\n",
        "clusters = get_clusters(PROJECT_ID, REGION)\n",
        "if clusters:\n",
        "    # @markdown Run this cell to display the Cluster dropdown menu:\n",
        "    clusters_with_prompt = [\"Select Cluster\"] + clusters\n",
        "    cluster_dropdown = widgets.Dropdown(\n",
        "        options=clusters_with_prompt, description=\"Clusters\", disabled=False\n",
        "    )\n",
        "    cluster_dropdown.observe(on_cluster_change, names=\"value\")\n",
        "    display(cluster_dropdown)\n",
        "else:\n",
        "    print(f\"No clusters found in {PROJECT_ID}/{REGION}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKGTaN84p8rX",
      "metadata": {
        "cellView": "form",
        "id": "IKGTaN84p8rX"
      },
      "outputs": [],
      "source": [
        "# @title # Chat completion for text-only models {run:\"auto\", vertical-output: true}\n",
        "\n",
        "# @markdown You may send prompts to the model server for prediction.\n",
        "# @markdown\n",
        "# @markdown * **user_prompt (string):** This is the text prompt you provide to the language model. It's the question or instruction e (e.g., \"Explain neural networks\").\n",
        "\n",
        "# @markdown * **temperature (number):** This  parameter controls the randomness of the model's output. It influences how the model selects the next token in the sequence it generates. Typical values range from 0.2 to 1.0.\n",
        "\n",
        "# @markdown * **max_tokens (number):** This parameter refers to the maximum number of tokens (words or sub-word units) that the model is allowed to generate in its response.\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def get_deployment_pod_name(deployment):\n",
        "    try:\n",
        "        label = deployment + \"-app\"\n",
        "        pods = json.loads(\n",
        "            subprocess.run(\n",
        "                [\"kubectl\", \"get\", \"pods\", \"-o\", \"json\", \"-l\", f\"app={label}\"],\n",
        "                capture_output=True,\n",
        "                check=True,\n",
        "            ).stdout\n",
        "        )\n",
        "        return pods[\"items\"][0][\"metadata\"][\"name\"] if pods[\"items\"] else None\n",
        "    except (\n",
        "        subprocess.CalledProcessError,\n",
        "        json.JSONDecodeError,\n",
        "        KeyError,\n",
        "        IndexError,\n",
        "    ):\n",
        "        return None\n",
        "\n",
        "\n",
        "def check_vllm_label(pod_name):\n",
        "    \"\"\"Checks if the pod has the 'ai.gke.io/inference-server=vllm' label.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"kubectl\", \"get\", \"pod\", pod_name, \"-o\", \"json\"],\n",
        "            capture_output=True,\n",
        "            check=True,\n",
        "        )\n",
        "        labels = json.loads(result.stdout)[\"metadata\"][\"labels\"]\n",
        "        return labels.get(\"ai.gke.io/inference-server\") == \"vllm\"\n",
        "    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError):\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_response(request, pod_name, pod_endpoint, is_vllm):\n",
        "    response = !kubectl exec -t {pod_name} -- curl -X POST http://{pod_endpoint}/generate -H \"Content-Type: application/json\" -d '{json.dumps(request)}' 2> /dev/null\n",
        "    try:\n",
        "        data = json.loads(response[0])\n",
        "        if is_vllm:\n",
        "            return data[\"predictions\"][0]\n",
        "        else:\n",
        "            return data[\"generated_text\"]\n",
        "    except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
        "        return f\"Error: {e}, Raw: {response}\"\n",
        "\n",
        "\n",
        "deployment_pod = get_deployment_pod_name(SELECTED_DEPLOYMENT)\n",
        "is_vllm_inference = check_vllm_label(deployment_pod)\n",
        "\n",
        "user_prompt = \"What is AI?\"  # @param {type: \"string\"}\n",
        "temperature = 0.50  # @param {type: \"number\"}\n",
        "max_tokens = 250  # @param {type: \"number\"}\n",
        "\n",
        "request = {\n",
        "    \"max_tokens\": 250 if max_tokens is None else max_tokens,\n",
        "    \"temperature\": 0.5 if temperature is None else temperature,\n",
        "}\n",
        "\n",
        "if is_vllm_inference:\n",
        "    request[\"prompt\"] = user_prompt\n",
        "else:\n",
        "    request[\"inputs\"] = user_prompt\n",
        "\n",
        "model_service = SELECTED_DEPLOYMENT + \"-service\"\n",
        "output = !kubectl get endpoints {model_service}\n",
        "pod_endpoint = output[1].split()[1]\n",
        "\n",
        "# @markdown ### Response:\n",
        "response = process_response(request, deployment_pod, pod_endpoint, is_vllm_inference)\n",
        "HTML(\n",
        "    '<div style=\"overflow-x: auto; font-size: 16px; line-height:'\n",
        "    f' 1.8;\">{response}</div>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6ZM2K3fux0",
      "metadata": {
        "id": "5b6ZM2K3fux0"
      },
      "source": [
        "# Next Steps: Integrating the GKE Service Endpoint\n",
        "\n",
        "After successfully deploying a model on Google Kubernetes Engine (GKE) and verifying it via a notebook, the next step is to integrate it into various applications. This involves making HTTP requests to the service's endpoint from your application code.\n",
        "\n",
        "### Exposing the Service\n",
        "\n",
        "To make your deployed model accessible to applications, you'll need to expose its service endpoint. Google Kubernetes Engine offers several ways to do this:\n",
        "\n",
        "1.  **Ingress:** Configure an Ingress resource to route external HTTP(S) traffic to your service. Set up Ingress for either an internal Load Balancer (accessible only within your VPC) or an external Load Balancer (accessible from the internet). [Learn more about GKE Ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress).\n",
        "2.  **Gateway API:** A more modern and feature-rich API for managing traffic routing in Kubernetes. Similar to Ingress, Gateway API allows you to define how external and internal traffic should be directed to your services. [Explore GKE Gateway API](https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api).\n",
        "\n",
        "### Setting Up Autoscaling\n",
        "\n",
        "Ensure your model serving can handle varying traffic by configuring the Horizontal Pod Autoscaler (HPA). HPA automatically scales the number of Pods based on resource utilization or custom metrics, optimizing performance and cost. [See how to configure HPA](https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling).\n",
        "\n",
        "### Setting Up Monitoring\n",
        "\n",
        "Monitor the health and performance of your deployed model using Google Cloud Managed Service for Prometheus. Configure your model serving to expose Prometheus metrics for comprehensive insights. [Get started with Google Cloud Managed Prometheus](https://cloud.google.com/kubernetes-engine/docs/how-to/configure-automatic-application-monitoring).\n",
        "\n",
        "### Additional Resources:\n",
        "\n",
        "* #### Kubernetes Documentation:\n",
        "   * Services: https://kubernetes.io/docs/concepts/services-networking/service/\n",
        "\n",
        "* #### Google Cloud Documentation:\n",
        "   * Google Kubernetes Engine (GKE): https://cloud.google.com/kubernetes-engine\n",
        "   * Cloud Load Balancing: https://cloud.google.com/load-balancing/docs/ingress\n",
        "   * Gateway API on GKE: https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api\n",
        "   * Learn about GPUs in GKE: https://cloud.google.com/kubernetes-engine/docs/concepts/gpus\n",
        "\n",
        "* #### Python requests Library:\n",
        "   * https://requests.readthedocs.io/en/latest/\n",
        "\n",
        "* #### LangChain with Google Integrations:\n",
        "   * The Langchain documentation is very useful: https://python.langchain.com/docs/integrations/providers/google/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gke_model_ui_deployment_notebook.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
