{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "20qcPG1PmFUM"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXYOa1odnikj"
      },
      "source": [
        "# Vertex AI Model Garden Integration With ADK\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_integration_with_adk.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"\u003e\u003cbr\u003e Run in Colab Enterprise\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_integration_with_adk.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbDI9ag4oR4C"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an agent with a Model Garden open model deployed through Vertex endpoint and [Google Agent Development Kit](https://google.github.io/adk-docs/) (ADK). The agent can automatically call function tools like `get_weather` and `get_current_time`.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy Vertex AI Model Garden OSS LLMs properly for ADK integration\n",
        "- Test deployed endpoints\n",
        "- Build agent web apps with deployed endpoints and ADK\n",
        "- Deploy agent web apps to Cloud Run\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQJWRopioSKT"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_jmxcIZoSxU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown \u003e | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform\u003e=1.84.0'\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# Import the necessary packages\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "if not os.path.exists(\"./vertex-ai-samples\"):\n",
        "  ! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "from etils import epath\n",
        "TUTORIAL_DIR = epath.Path(\"vmg_adk_agent_tutorial\")\n",
        "BUILD_DIR = TUTORIAL_DIR / \"build\"\n",
        "BUILD_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "REPOSITORY_NAME = \"vertex-vision-model-garden-dockers\"\n",
        "\n",
        "!gcloud artifacts repositories create $REPOSITORY_NAME \\\n",
        "      --repository-format=docker \\\n",
        "      --location=$REGION \\\n",
        "      --project=$PROJECT_ID\n",
        "\n",
        "! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate OSS LLMs With ADK"
      ],
      "metadata": {
        "id": "T9UiFwbiowOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy OSS LLMs For Agents\n",
        "# @markdown In order to use OSS LLM endpoints smoothly with ADK,\n",
        "# @markdown these models should be deployed with:\n",
        "\n",
        "# @markdown - **enabling tool calls**. e.g.:\n",
        "# @markdown If the models are deployed with *vllm*,\n",
        "# @markdown the deployment should specify settings like `--enable-auto-tool-choice`\n",
        "# @markdown and `--tool-call-parser=hermes`.\n",
        "# @markdown If the models are deployed with *sglang*, the deployment should specify\n",
        "# @markdown setting like `--tool-call-parser=qwen25`.\n",
        "# @markdown Refer to tool calls in\n",
        "# @markdown [vllm](https://docs.vllm.ai/en/stable/features/tool_calling.html)\n",
        "# @markdown and [sglang](https://docs.sglang.ai/backend/function_calling.html) for more details.\n",
        "\n",
        "# @markdown - **disable dedicated endpoints**. The dedicated endpoints are not\n",
        "# @markdown supported in ADK yet.\n",
        "\n",
        "# @markdown You can deploy models below with proper deployment settings for ADK integration.\n",
        "\n",
        "MODEL_ID = \"Qwen3-32B\"  # @param [\"Qwen3-32B\"] {isTemplate: true}\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_L4\", \"NVIDIA_A100_80GB\", \"NVIDIA_H100_80GB\"] {isTemplate: true}\n",
        "\n",
        "if accelerator_type == \"NVIDIA_L4\":\n",
        "    accelerator_count = 4\n",
        "    # Sets machine type to g2-standard-48 for 4 L4's\n",
        "    machine_type = \"g2-standard-48\"\n",
        "elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    accelerator_count = 1\n",
        "    # Sets machine type to a2-ultragpu-1g for 1 Nvidia A100 80 GB.\n",
        "    machine_type = \"a2-ultragpu-1g\"\n",
        "elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 2\n",
        "    machine_type = \"a3-highgpu-2g\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "\n",
        "deploy_request_timeout = 1800  # 30 minutes\n",
        "\n",
        "from vertexai.preview import model_garden\n",
        "publisher_model_name = f\"publishers/qwen/models/qwen3@{MODEL_ID.lower()}\"\n",
        "model = model_garden.OpenModel(publisher_model_name)\n",
        "\n",
        "container_spec = model.list_deploy_options()[0].container_spec\n",
        "updated_args = container_spec.args[:-2] + [f\"--tp={accelerator_count}\", \"--tool-call-parser=qwen25\"]\n",
        "container_spec.args = updated_args\n",
        "\n",
        "print(\"The container spec are:\")\n",
        "print(container_spec)\n",
        "\n",
        "print(\"Start to check quota for the deployment.\")\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "print(\"Finished to check quota for the deployment.\")\n",
        "\n",
        "print(\"Start to deploy models to endpoints.\")\n",
        "endpoint = model.deploy(\n",
        "    serving_container_spec=container_spec,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=False,\n",
        "    spot=False,\n",
        "    deploy_request_timeout=deploy_request_timeout,\n",
        "    accept_eula=False,\n",
        ")\n",
        "print(\"Finished to deploy models to endpoints.\")\n",
        "# @markdown After endpoints are deployed successfully, you get the endpoint\n",
        "# @markdown resource name with the format as\n",
        "# @markdown `projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}`.\n",
        "# @markdown The endpoint resource name will be used in local predictions and\n",
        "# @markdown integration with ADK below.\n",
        "endpoint_resource_name = endpoint.resource_name\n",
        "print(\"The deployed endpoint resource name is:\")\n",
        "print(endpoint_resource_name)\n",
        "# @markdown Click \"Show Code\" to see more details.\n"
      ],
      "metadata": {
        "id": "gbfFZLT5KkUV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test The Endpoint\n",
        "# endpoint_resource_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_NAME}\"\n",
        "# )\n",
        "endpoint = aiplatform.Endpoint(endpoint_resource_name)\n",
        "\n",
        "location = endpoint_resource_name.split('/')[3]\n",
        "base_url = f\"https://{location}-aiplatform.googleapis.com/v1beta1/{endpoint.resource_name}\"\n",
        "\n",
        "# @markdown Predict locally with some requests.\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 100  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = True  # @param {type: \"boolean\"}\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "client = openai.OpenAI(base_url=base_url, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_MNVrfJomCd0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa4e1-6FvRAP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Build Agent Web App Dockers With VMG Endpoints\n",
        "# @markdown The section will create required python and docker files first, and\n",
        "# @markdown then build the dockers with cloud build.\n",
        "\n",
        "# @markdown 1. Create `agent.py` by loading VMG endpoints and example tool functions.\n",
        "agent_app = '''\n",
        "\"\"\"This is a sample agent for model garden agents.\"\"\"\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import zoneinfo\n",
        "\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "import google.auth\n",
        "\n",
        "_MODEL_GARDEN_ENDPOINT_REGEX = r\"projects\\/.+\\/locations\\/.+\\/endpoints\\/.+\"\n",
        "\n",
        "\n",
        "def get_weather(city: str) -\u003e str:\n",
        "  \"\"\"Simulates a web search. Use it get information on weather.\n",
        "\n",
        "  Args:\n",
        "      city: A string containing the location to get weather information for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the simulated weather information for the queried city.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    return \"It's 70 degrees and foggy.\"\n",
        "  return \"It's 80 degrees and sunny.\"\n",
        "\n",
        "\n",
        "def get_current_time(city: str) -\u003e str:\n",
        "  \"\"\"Simulates getting the current time for a city.\n",
        "\n",
        "  Args:\n",
        "      city: The name of the city to get the current time for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the current time information.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    tz_identifier = \"America/Los_Angeles\"\n",
        "  else:\n",
        "    return f\"Sorry, I don't have timezone information for city: {city}.\"\n",
        "\n",
        "  tz = zoneinfo.ZoneInfo(tz_identifier)\n",
        "  now = datetime.datetime.now(tz)\n",
        "  return (\n",
        "      f\"The current time for city {city} is\"\n",
        "      f\" {now.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\"\n",
        "  )\n",
        "\n",
        "\n",
        "def _get_auth_headers() -\u003e dict[str, str]:\n",
        "  \"\"\"Gets the auth headers for the model garden endpoint.\"\"\"\n",
        "  creds, _ = google.auth.default(\n",
        "      scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "  )\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  return {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {creds.token}\",\n",
        "  }\n",
        "\n",
        "\n",
        "def _setup_model_garden_endpoint():\n",
        "  \"\"\"Sets up the model garden endpoint.\"\"\"\n",
        "  endpoint = os.environ.get(\"GOOGLE_MODEL_GARDEN_ENDPOINT\", \"\")\n",
        "\n",
        "  if not re.compile(_MODEL_GARDEN_ENDPOINT_REGEX).fullmatch(endpoint):\n",
        "    raise ValueError(\n",
        "        f\"Invalid model garden endpoint: {endpoint}. Please use the format\"\n",
        "        \" projects/{project}/locations/{location}/endpoints/{endpoint}.\"\n",
        "    )\n",
        "  endpoint_parts = endpoint.split(\"/\")\n",
        "  os.environ.setdefault(\"GOOGLE_GENAI_USE_VERTEXAI\", \"True\")\n",
        "  os.environ[\"VERTEXAI_PROJECT\"] = endpoint_parts[1]\n",
        "  os.environ[\"VERTEXAI_LOCATION\"] = endpoint_parts[3]\n",
        "  os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
        "  return f\"vertex_ai/openai/{endpoint_parts[5]}\"\n",
        "\n",
        "\n",
        "auth_headers = _get_auth_headers()\n",
        "model = _setup_model_garden_endpoint()\n",
        "\n",
        "print(\"The current model is: {model}\")\n",
        "\n",
        "root_agent = LlmAgent(\n",
        "    name=\"root_agent\",\n",
        "    model=LiteLlm(\n",
        "        model=model,\n",
        "        extra_headers=auth_headers,\n",
        "    ),\n",
        "    instruction=(\n",
        "        \"You are a helpful AI assistant designed to provide accurate and useful\"\n",
        "        \" information. Please output the tool callings with json format if\"\n",
        "        \" exists.\"\n",
        "    ),\n",
        "    description=\"Retrieves the weather and current time using specific tools.\",\n",
        "    tools=[get_weather, get_current_time],\n",
        ")\n",
        "'''\n",
        "with BUILD_DIR.joinpath(\"agent.py\").open(\"w\") as f:\n",
        "    f.write(agent_app)\n",
        "\n",
        "# @markdown 2. Create `__init__.py` to load agent.py for ADK apps.\n",
        "initialize = '''\n",
        "from . import agent\n",
        "'''\n",
        "with BUILD_DIR.joinpath(\"__init__.py\").open(\"w\") as f:\n",
        "    f.write(initialize)\n",
        "\n",
        "# @markdown 3. Create `Dockerfile` to build agent app dockers.\n",
        "dockerfile_content = '''\n",
        "FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "RUN adduser --disabled-password --gecos \"\" myuser\n",
        "RUN chown -R myuser:myuser /app\n",
        "USER myuser\n",
        "ENV PATH=\"/home/myuser/.local/bin:$PATH\"\n",
        "RUN pip install \\\n",
        "  google-adk~=0.4.0 \\\n",
        "  google-cloud-logging~=3.11.4 \\\n",
        "  opentelemetry-exporter-gcp-trace~=1.9.0 \\\n",
        "  google-cloud-aiplatform[evaluation,agent-engines]~=1.88.0 \\\n",
        "  litellm~=1.66.2\n",
        "\n",
        "COPY agent.py \"/app/agents/model_garden_agents/\"\n",
        "COPY __init__.py \"/app/agents/model_garden_agents/\"\n",
        "ENV GOOGLE_MODEL_GARDEN_ENDPOINT YOUR_ENDPOINT\n",
        "EXPOSE 8000\n",
        "CMD adk web --port=8000 --trace_to_cloud \"/app/agents\"\n",
        "'''\n",
        "with BUILD_DIR.joinpath(\"Dockerfile\").open(\"w\") as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "# @markdown 4. Build agent web app dockers.\n",
        "VMG_AGENT_UI_CONTAINER_IMAGE_URI = (\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY_NAME}/vmg-adk-ui\"\n",
        ")\n",
        "! gcloud builds submit --tag $VMG_AGENT_UI_CONTAINER_IMAGE_URI --project $PROJECT_ID --machine-type e2-highcpu-32 $BUILD_DIR\n",
        "print(\"The agent UI docker is :\")\n",
        "print(VMG_AGENT_UI_CONTAINER_IMAGE_URI)\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy Agent Web App Dockers To Cloud Run\n",
        "# @markdown After the deployment, there will be a service URL. You can click\n",
        "# @markdown the service URL and interact with the agent web app. The agent web\n",
        "# @markdown is a built-in development UI in [ADK](https://github.com/google/adk-python?tab=readme-ov-file)\n",
        "# @markdown to help you test, evaluate, debug, and showcase your agent(s).\n",
        "\n",
        "# @markdown ![ADK WEB UI](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)\n",
        "\n",
        "! gcloud run deploy vmg-agent-ui-1 \\\n",
        "    --port 8000 \\\n",
        "    --image=\"{VMG_AGENT_UI_CONTAINER_IMAGE_URI}\" \\\n",
        "    --region=\"{REGION}\" \\\n",
        "    --platform=managed \\\n",
        "    --allow-unauthenticated \\\n",
        "    --memory=1024Mi \\\n",
        "    --set-env-vars=\"GOOGLE_MODEL_GARDEN_ENDPOINT={endpoint_resource_name}\"\n",
        "# @markdown Click \"Show Code\" to see more details."
      ],
      "metadata": {
        "id": "L2AGmQZmuVam",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAelDidov5AW"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SeZCFo5v7z-"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment resources to avoid unnecessary continuous\n",
        "# @markdown  charges that may incur.\n",
        "\n",
        "delete_endpoint = False # @param {type:\"boolean\"}\n",
        "delete_artifact_registry = False # @param {type:\"boolean\"}\n",
        "delete_tutorial_folder = False # @param {type:\"boolean\"}\n",
        "\n",
        "if delete_endpoint:\n",
        "  # Undeploy model and delete endpoint.\n",
        "  endpoint.delete(force=True)\n",
        "\n",
        "if delete_artifact_registry:\n",
        "    ! gcloud artifacts repositories delete $REPOSITORY_NAME \\\n",
        "          --repository-format=docker \\\n",
        "          --location=$REGION \\\n",
        "          --project=$PROJECT_ID\n",
        "\n",
        "if delete_tutorial_folder:\n",
        "    import shutil\n",
        "    shutil.rmtree(TUTORIAL_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_integration_with_adk.ipynb",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "tAelDidov5AW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
