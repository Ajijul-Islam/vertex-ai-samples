{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pr9TgOcV9vAXeqGiyTaTI5kS",
      "metadata": {
        "cellView": "form",
        "id": "Pr9TgOcV9vAXeqGiyTaTI5kS"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M1CpgYundFwz",
      "metadata": {
        "id": "M1CpgYundFwz"
      },
      "source": [
        "# Get started with your deployed model on GKE\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fgke_model_ui_deployment_notebook_auto.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/gke_model_ui_deployment_notebook_auto.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t2jj2XOgkS4F",
      "metadata": {
        "id": "t2jj2XOgkS4F"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook will guide you through the initial step of testing your recently\n",
        "deployed model with text prompts. Depending on your deployed model's inference\n",
        "setup, the notebook utilizes either Text Generation Inference\n",
        "[TGI](https://huggingface.co/docs/text-generation-inference/en/index) or\n",
        "[vLLM](https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/#:~:text=model%20frameworks%20simple.-,What%20is%20vLLM%3F,-vLLM%20is%20an),\n",
        "two efficient serving frameworks that enhance the performance of your GPU model.\n",
        "Ready to see your deployed model respond? Run the cells below and start\n",
        "experimenting with different prompts!\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before proceeding with this notebook, ensure you have already deployed a model\n",
        "using the Google Cloud Console. You can find an overview of AI and Machine\n",
        "Learning services on\n",
        "[GKE AI/ML](https://console.cloud.google.com/kubernetes/aiml/overview).\n",
        "\n",
        "### Objective\n",
        "\n",
        "Enable prompt-based testing of the AI model deployed on GKE\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes, such as\n",
        "machine learning and data processing. GKE provides a range of machine type\n",
        "options for node configuration, including machine types with NVIDIA H100, L4,\n",
        "and A100 GPUs.\n",
        "\n",
        "### Understanding the Inference Frameworks\n",
        "\n",
        "Your model is running on one of two popular and efficient serving frameworks:\n",
        "vLLM or Text Generation Inference (TGI). The following sections provide a brief\n",
        "overview of each to give you context on the underlying technology powering your\n",
        "model.\n",
        "\n",
        "#### TGI\n",
        "\n",
        "TGI is a highly optimized open-source LLM serving framework that can increase\n",
        "serving throughput on GPUs. TGI includes features such as:\n",
        "\n",
        "*   Optimized transformer implementation with PagedAttention\n",
        "*   Continuous batching to improve the overall serving throughput\n",
        "*   Tensor parallelism and distributed serving on multiple GPUs\n",
        "\n",
        "To learn more, refer to the\n",
        "[TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md)\n",
        "\n",
        "#### vLLM\n",
        "\n",
        "vLLM is another fast and easy-to-use library for LLM inference and serving. It's\n",
        "known for its high throughput and efficiency, and it leverages PagedAttention.\n",
        "Key features include:\n",
        "\n",
        "*   PagedAttention: Efficient memory management for handling long sequences and\n",
        "    dynamic workloads.\n",
        "*   Continuous batching: Maximizes GPU utilization by batching incoming\n",
        "    requests.\n",
        "*   High-throughput serving: Designed for production-level serving with low\n",
        "    latency.\n",
        "*   Optimized CUDA kernels.\n",
        "\n",
        "To learn more, refer to the\n",
        "[vLLM documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMf-T58TkDy1",
      "metadata": {
        "cellView": "form",
        "id": "XMf-T58TkDy1"
      },
      "outputs": [],
      "source": [
        "# @title # Connect to Google Cloud Project\n",
        "# @markdown #### Run this cell to configure your Google Cloud environment for Kubernetes (GKE) operations.\n",
        "# @markdown\n",
        "# @markdown #### Actions:\n",
        "# @markdown 1.  **Connects to Project:** Retrieves and sets your Google Cloud project ID.\n",
        "# @markdown 3.  **Installs `kubectl`:** Installs the Kubernetes command-line tool.\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKGTaN84p8rX",
      "metadata": {
        "cellView": "form",
        "id": "IKGTaN84p8rX"
      },
      "outputs": [],
      "source": [
        "# @title # Chat completion for text-only models { vertical-output: true}\n",
        "# @markdown You may send prompts to the model server for prediction.\n",
        "# @markdown\n",
        "# @markdown * **user_prompt (string):** This is the text prompt you provide to the language model. It's the question or instruction e (e.g., \"Explain neural networks\").\n",
        "# @markdown * **temperature (number):** This  parameter controls the randomness of the model's output. It influences how the model selects the next token in the sequence it generates. Typical values range from 0.2 to 1.0.\n",
        "# @markdown * **max_tokens (number):** This parameter refers to the maximum number of tokens (words or sub-word units) that the model is allowed to generate in its response.\n",
        "# @markdown\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Markdown, clear_output, display\n",
        "\n",
        "CLUSTER = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "REGION = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "NAMESPACE = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "DEPLOYMENT = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "DEPLOYMENT_APP_LABEL = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "\n",
        "SERVICE = f\"{DEPLOYMENT}-service\"\n",
        "\n",
        "\n",
        "def _run_kubectl(cmd):\n",
        "    \"\"\"Executes a kubectl command and returns its stdout.\"\"\"\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=60)\n",
        "    return result.stdout.strip()\n",
        "\n",
        "\n",
        "def fetch_cluster_credential(cluster, region, project_id):\n",
        "    try:\n",
        "        # Ensure credentials for the target cluster\n",
        "        cred_cmd = [\n",
        "            \"gcloud\",\n",
        "            \"container\",\n",
        "            \"clusters\",\n",
        "            \"get-credentials\",\n",
        "            cluster,\n",
        "            f\"--location={region}\",\n",
        "            f\"--project={project_id}\",\n",
        "        ]\n",
        "        _run_kubectl(cred_cmd)\n",
        "    except Exception as e:\n",
        "        # Original code prints error and returns empty dict\n",
        "        print(f\"Error fetching cluster credentials: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "def get_deployment_pod_name(deployment, namespace, deployment_app_label):\n",
        "    \"\"\"Finds the running pod name for a given deployment and namespace.\"\"\"\n",
        "\n",
        "    cmd = [\n",
        "        \"kubectl\",\n",
        "        \"get\",\n",
        "        \"pods\",\n",
        "        \"-n\",\n",
        "        namespace,\n",
        "        \"-o\",\n",
        "        \"json\",\n",
        "        \"-l\",\n",
        "        f\"app={deployment_app_label}\",\n",
        "        \"--field-selector=status.phase=Running\",\n",
        "    ]\n",
        "    try:\n",
        "        pods_json = _run_kubectl(cmd)\n",
        "        pods = json.loads(pods_json)\n",
        "        if pods.get(\"items\"):\n",
        "            return pods[\"items\"][0][\"metadata\"][\"name\"]\n",
        "        print(f\"No running pods found for {deployment} in {namespace}.\")\n",
        "        return None\n",
        "    except (\n",
        "        subprocess.CalledProcessError,\n",
        "        json.JSONDecodeError,\n",
        "        IndexError,\n",
        "        KeyError,\n",
        "    ) as e:\n",
        "        print(f\"Error getting pod name for {deployment} in {namespace}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def check_inference_label(pod_name, namespace):\n",
        "    \"\"\"Checks if the specified pod has the vLLM inference server label.\"\"\"\n",
        "\n",
        "    cmd = [\"kubectl\", \"get\", \"pod\", pod_name, \"-n\", namespace, \"-o\", \"json\"]\n",
        "    try:\n",
        "        pod_json = _run_kubectl(cmd)\n",
        "        labels = json.loads(pod_json).get(\"metadata\", {}).get(\"labels\", {})\n",
        "        return labels.get(\"ai.gke.io/inference-server\") == \"vllm\"\n",
        "    except (subprocess.CalledProcessError, json.JSONDecodeError, KeyError) as e:\n",
        "        print(f\"Error checking labels for pod {pod_name} in {namespace}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_service_endpoint(service, namespace):\n",
        "    \"\"\"Retrieve the service endpoint of the deployment\"\"\"\n",
        "    endpoint_cmd = [\n",
        "        \"kubectl\",\n",
        "        \"get\",\n",
        "        \"endpoints\",\n",
        "        service,\n",
        "        \"-n\",\n",
        "        namespace,\n",
        "    ]\n",
        "    try:\n",
        "        endpoint_output = _run_kubectl(endpoint_cmd).splitlines()\n",
        "        if len(endpoint_output) < 2 or len(endpoint_output[1].split()) < 2:\n",
        "            print(f\"Endpoint data incomplete for {service}.\")\n",
        "            return None\n",
        "        endpoint = endpoint_output[1].split()[\n",
        "            1\n",
        "        ]  # Assumes format: NAME ENDPOINTS AGE -> service ip:port,... age\n",
        "        return endpoint\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error getting endpoints for {service}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_response(request, pod_name, pod_endpoint, is_vllm_inference, namespace):\n",
        "    \"\"\"Sends a request to the pod and processes the response.\"\"\"\n",
        "\n",
        "    json_data_escaped = json.dumps(request).replace(\"'\", \"'\\\\''\")\n",
        "    curl_cmd = (\n",
        "        f\"kubectl exec -n {namespace} -t {pod_name} -- curl -s -X POST\"\n",
        "        f' http://{pod_endpoint}/generate -H \"Content-Type: application/json\"'\n",
        "        f\" -d '{json_data_escaped}' 2> /dev/null\"\n",
        "    )\n",
        "    try:\n",
        "        response_raw = _run_kubectl([\"bash\", \"-c\", curl_cmd])\n",
        "        if not response_raw:\n",
        "            return f\"Error: Empty response from pod {pod_name}.\"\n",
        "        first_line = response_raw.splitlines()[0]\n",
        "        data = json.loads(first_line)\n",
        "\n",
        "        if is_vllm_inference:  # vLLM format\n",
        "            predictions = data.get(\"predictions\")\n",
        "            if isinstance(predictions, (list, tuple)) and predictions:\n",
        "                return predictions[0]\n",
        "            return f\"Error: Unexpected vLLM format. Raw: {first_line}\"\n",
        "        else:  # TGI format\n",
        "            generated_text = data.get(\"generated_text\")\n",
        "            if generated_text is not None:\n",
        "                return generated_text\n",
        "            return f\"Error: Unexpected TGI format. Raw: {first_line}\"\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error during response processing: {e}\"\n",
        "\n",
        "\n",
        "# --- Widgets Setup ---\n",
        "user_prompt_widget = widgets.Textarea(\n",
        "    value=\"What is AI?\",\n",
        "    description=\"User Prompt:\",\n",
        "    layout=widgets.Layout(width=\"95%\", height=\"100px\"),\n",
        ")\n",
        "\n",
        "temperature_widget = widgets.FloatSlider(\n",
        "    value=0.50, min=0.0, max=1.0, step=0.01, description=\"Temperature:\"\n",
        ")\n",
        "\n",
        "max_tokens_widget = widgets.IntSlider(\n",
        "    value=250, min=1, max=2048, step=1, description=\"Max Tokens:\"\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "output_area_response = widgets.Output()\n",
        "\n",
        "\n",
        "# --- Submit Button Logic ---\n",
        "def on_submit_clicked(b):\n",
        "    \"\"\"Handles the submit button click event.\"\"\"\n",
        "    with output_area_response:\n",
        "        clear_output()\n",
        "\n",
        "        fetch_cluster_credential(CLUSTER, REGION, PROJECT_ID)\n",
        "\n",
        "        # retrieve deployment pod\n",
        "        pod_name = get_deployment_pod_name(DEPLOYMENT, NAMESPACE, DEPLOYMENT_APP_LABEL)\n",
        "        if not pod_name:\n",
        "            display(\n",
        "                Markdown(f\"**Error:** Could not find running pod for `{DEPLOYMENT}`.\")\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # build the request message\n",
        "        is_vllm = check_inference_label(pod_name, NAMESPACE)\n",
        "        request = {\n",
        "            \"max_tokens\": max_tokens_widget.value,\n",
        "            \"temperature\": temperature_widget.value,\n",
        "            \"prompt\" if is_vllm else \"inputs\": user_prompt_widget.value,\n",
        "        }\n",
        "\n",
        "        # retrieve service endpoint for the deployment\n",
        "        endpoint = get_service_endpoint(SERVICE, NAMESPACE)\n",
        "        if not endpoint:\n",
        "            display(Markdown(f\"**Error getting endpoints for `{SERVICE}`:**\\n\"))\n",
        "            return\n",
        "\n",
        "        # prompt test the deployment endpoint\n",
        "        try:\n",
        "            response = process_response(request, pod_name, endpoint, is_vllm, NAMESPACE)\n",
        "            display(Markdown(f\"**Response:**\\n\\n{response}\"))\n",
        "        except Exception as e:\n",
        "            display(Markdown(f\"**Unexpected Error:**\\n```\\n{e}\\n```\"))\n",
        "\n",
        "\n",
        "# --- Display Widgets ---\n",
        "submit_button.on_click(on_submit_clicked)\n",
        "display(\n",
        "    user_prompt_widget,\n",
        "    temperature_widget,\n",
        "    max_tokens_widget,\n",
        "    submit_button,\n",
        "    output_area_response,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6ZM2K3fux0",
      "metadata": {
        "id": "5b6ZM2K3fux0"
      },
      "source": [
        "# Next Steps: Integrating the GKE Service Endpoint\n",
        "\n",
        "After successfully deploying a model on Google Kubernetes Engine (GKE) and\n",
        "verifying it via a notebook, the next step is to integrate it into various\n",
        "applications. This involves making HTTP requests to the service's endpoint from\n",
        "your application code.\n",
        "\n",
        "### Exposing the Service\n",
        "\n",
        "To make your deployed model accessible to applications, you'll need to expose\n",
        "its service endpoint. Google Kubernetes Engine offers several ways to do this:\n",
        "\n",
        "1.  **Ingress:** Configure an Ingress resource to route external HTTP(S) traffic\n",
        "    to your service. Set up Ingress for either an internal Load Balancer\n",
        "    (accessible only within your VPC) or an external Load Balancer (accessible\n",
        "    from the internet).\n",
        "    [Learn more about GKE Ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress).\n",
        "2.  **Gateway API:** A more modern and feature-rich API for managing traffic\n",
        "    routing in Kubernetes. Similar to Ingress, Gateway API allows you to define\n",
        "    how external and internal traffic should be directed to your services.\n",
        "    [Explore GKE Gateway API](https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api).\n",
        "\n",
        "### Setting Up Autoscaling\n",
        "\n",
        "Ensure your model serving can handle varying traffic by configuring the\n",
        "Horizontal Pod Autoscaler (HPA). HPA automatically scales the number of Pods\n",
        "based on resource utilization or custom metrics, optimizing performance and\n",
        "cost.\n",
        "[See how to configure HPA](https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling).\n",
        "\n",
        "### Setting Up Monitoring\n",
        "\n",
        "Monitor the health and performance of your deployed model using Google Cloud\n",
        "Managed Service for Prometheus. Configure your model serving to expose\n",
        "Prometheus metrics for comprehensive insights.\n",
        "[Get started with Google Cloud Managed Prometheus](https://cloud.google.com/kubernetes-engine/docs/how-to/configure-automatic-application-monitoring).\n",
        "\n",
        "### Additional Resources:\n",
        "\n",
        "*   #### Kubernetes Documentation:\n",
        "\n",
        "    *   Services:\n",
        "        https://kubernetes.io/docs/concepts/services-networking/service/\n",
        "\n",
        "*   #### Google Cloud Documentation:\n",
        "\n",
        "    *   Google Kubernetes Engine (GKE):\n",
        "        https://cloud.google.com/kubernetes-engine\n",
        "    *   Cloud Load Balancing:\n",
        "        https://cloud.google.com/load-balancing/docs/ingress\n",
        "    *   Gateway API on GKE:\n",
        "        https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api\n",
        "    *   Learn about GPUs in GKE:\n",
        "        https://cloud.google.com/kubernetes-engine/docs/concepts/gpus\n",
        "\n",
        "*   #### Python requests Library:\n",
        "\n",
        "    *   https://requests.readthedocs.io/en/latest/\n",
        "\n",
        "*   #### LangChain with Google Integrations:\n",
        "\n",
        "    *   The Langchain documentation is very useful:\n",
        "        https://python.langchain.com/docs/integrations/providers/google/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gke_model_ui_deployment_notebook_auto.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
