{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - DeepSeek (Deployment)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_deepseek_deployment.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deepseek_deployment.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates serving DeepSeek models with [vLLM](https://github.com/vllm-project/vllm) or [SGLang](https://github.com/sgl-project/sglang). [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) is a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) is one of the first-generation reasoning models introduced by DeepSeek and offers performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy DeepSeek-V3 and DeepSeek-R1 with vLLM or SGLang on GPU using multi-host serving and [Spot VMs](https://cloud.google.com/compute/docs/instances/spot). Multi-host GPU serving is a preview feature.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "ax7zWynUDcjk"
      },
      "outputs": [],
      "source": [
        "# @title Request for quota\n",
        "\n",
        "# @markdown To deploy the largest variants of the DeepSeek models, you need 2 hosts of 8 x H100 machines, which gives a total of 16 H100s. Check that you have sufficient Spot VM quota: [`CustomModelServingPreemptibleH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_preemptible_nvidia_h100_gpus). If not, request for H100 quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically to us-central1.\n",
        "\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with Spot VM H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for Spot VM H100 GPUs: [`CustomModelServingPreemptibleH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_preemptible_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, asia-southeast1 |\n",
        "\n",
        "# Import the necessary packages\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import importlib\n",
        "import os\n",
        "import time\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "from google import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "def check_quota(\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    resource_id: str,\n",
        "    accelerator_count: int,\n",
        "):\n",
        "    \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
        "    quota = common_util.get_quota(project_id, region, resource_id)\n",
        "    quota_request_instruction = (\n",
        "        \"Either use \"\n",
        "        \"a different region or request additional quota. Follow \"\n",
        "        \"instructions here \"\n",
        "        \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
        "        \" to check quota in a region or request additional quota for \"\n",
        "        \"your project.\"\n",
        "    )\n",
        "    if quota == -1:\n",
        "        raise ValueError(\n",
        "            f\"Quota not found for: {resource_id} in {region}.\"\n",
        "            f\" {quota_request_instruction}\"\n",
        "        )\n",
        "    if quota < accelerator_count:\n",
        "        raise ValueError(\n",
        "            f\"Quota not enough for {resource_id} in {region}: {quota} <\"\n",
        "            f\" {accelerator_count}. {quota_request_instruction}\"\n",
        "        )\n",
        "\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XybZjtgF9M"
      },
      "source": [
        "## Deploy DeepSeek-V3 and DeepSeek-R1 with vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads DeepSeek models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# @markdown Multi-host GPU serving is a preview feature.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"DeepSeek-R1\"  # @param [\"DeepSeek-V3\", \"DeepSeek-V3-Base\", \"DeepSeek-R1\"] {isTemplate:true}\n",
        "model_id = \"deepseek-ai/\" + base_model_name\n",
        "hf_model_id = model_id\n",
        "if \"R1\" in model_id:\n",
        "    model_id = f\"gs://vertex-model-garden-restricted-us/{model_id}\"\n",
        "\n",
        "# @markdown vLLM v0.7.2 has been validated and is used for deployment here. The version will be continuously updated to incorporate latest optimizations and features, such as MLA with chunked prefill.\n",
        "# The pre-built serving docker image for vLLM v0.7.2.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250304_0916_RC01\"\n",
        "\n",
        "# @markdown Use a [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint) for the deployment.\n",
        "# @markdown Only dedicated endpoints are supported at this moment.\n",
        "use_dedicated_endpoint = True\n",
        "\n",
        "# @markdown Use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "# @markdown Only Spot VMs are supported at this moment.\n",
        "is_spot = True\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "accelerator_count = 8\n",
        "machine_type = \"a3-highgpu-8g\"\n",
        "multihost_gpu_node_count = 2\n",
        "resource_id = \"custom_model_serving_preemptible_nvidia_h100_gpus\"\n",
        "\n",
        "check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    resource_id=resource_id,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        ")\n",
        "\n",
        "# @markdown A context length of 4096 is supported in the following configuration. The configuration has been validated for stability and performance.\n",
        "# @markdown As the vLLM version gets updated in the future, more performant configurations will be offered. For example, support for MLA with chunked prefill will reduce the peak memory usage and allow for a larger context length under competitive performance.\n",
        "pipeline_parallel_size = 2\n",
        "gpu_memory_utilization = 0.8\n",
        "max_model_len = 4096  # Maximum context length.\n",
        "max_num_seqs = 64\n",
        "kv_cache_dtype = \"auto\"\n",
        "\n",
        "\n",
        "# The pre-built serving docker image and configuration for vLLM v0.6.6.post1.\n",
        "# VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250130_0916_RC01\"\n",
        "# gpu_memory_utilization = 0.9\n",
        "# max_model_len = 32768  # Maximum context length.\n",
        "# max_num_seqs = 128\n",
        "# kv_cache_dtype = \"fp8\"\n",
        "\n",
        "\n",
        "# Enable automatic prefix caching using GPU HBM\n",
        "enable_prefix_cache = False\n",
        "# Setting this value >0 will use the idle host memory for a second-tier prefix kv\n",
        "# cache beneath the HBM cache. It only has effect if enable_prefix_cache=True.\n",
        "# The range of this value: [0, 1)\n",
        "# Setting host_prefix_kv_cache_utilization_target to 0 will disable the host memory prefix kv cache.\n",
        "host_prefix_kv_cache_utilization_target = 0\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_vllm_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = None,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    pipeline_parallel_size: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    kv_cache_dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        "    is_spot: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={int(accelerator_count * multihost_gpu_node_count / pipeline_parallel_size)}\",\n",
        "        f\"--pipeline-parallel-size={pipeline_parallel_size}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--kv-cache-dtype={kv_cache_dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if multihost_gpu_node_count > 1:\n",
        "        vllm_args = [\"/vllm-workspace/ray_launcher.sh\"] + vllm_args\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"serviceAccount\": service_account,\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_deepseek_deployment.ipynb\",\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"deepseek-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"deepseek-ai\",\n",
        "    publisher_model_id=(\"deepseek-v3\" if \"V3\" in model_id else \"deepseek-r1\"),\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=None,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    pipeline_parallel_size=pipeline_parallel_size,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    max_num_seqs=max_num_seqs,\n",
        "    kv_cache_dtype=kv_cache_dtype,\n",
        "    enable_trust_remote_code=True,\n",
        "    enforce_eager=False,\n",
        "    enable_lora=False,\n",
        "    enable_chunked_prefill=False,\n",
        "    enable_prefix_cache=enable_prefix_cache,\n",
        "    host_prefix_kv_cache_utilization_target=host_prefix_kv_cache_utilization_target,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rDHsCOqvFYBi"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, so I need to figure out how to diagnose and fix a flickering light in my house. Hmm, where do I start? Let's think. First, I remember that flickering lights can be caused by various issues. Maybe the bulb is loose? That's a common problem. Let me start with the simplest things first.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-R1 model is shown below as an example.\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-V3 model would be: \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜>\\n\"\n",
        "prompt = \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜><think>\\n\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 1024  # @param {type:\"integer\"}\n",
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "top_p = 0.95  # @param {type:\"number\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LSG9ITWTbTb7"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"vllm_gpu\"].name\n",
        ")\n",
        "\n",
        "# @markdown Because the DeepSeek-R1 model generates detailed reasoning steps, the output is expected to be long. We recommend using streaming for a better generation experience.\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zJJDmldn7rw"
      },
      "source": [
        "## Deploy DeepSeek-V3 and DeepSeek-R1 with SGLang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_3Swj3pxn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads DeepSeek models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# @markdown Multi-host GPU serving is a preview feature.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"DeepSeek-R1\"  # @param [\"DeepSeek-V3\", \"DeepSeek-V3-Base\", \"DeepSeek-R1\"] {isTemplate:true}\n",
        "model_id = \"deepseek-ai/\" + base_model_name\n",
        "hf_model_id = model_id\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "SGLANG_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310\"\n",
        "\n",
        "# @markdown Use a [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint) for the deployment.\n",
        "# @markdown Only dedicated endpoints are supported at this moment.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "# @markdown Only Spot VMs are supported at this moment.\n",
        "is_spot = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "accelerator_count = 8\n",
        "machine_type = \"a3-highgpu-8g\"\n",
        "multihost_gpu_node_count = 2\n",
        "resource_id = \"custom_model_serving_preemptible_nvidia_h100_gpus\"\n",
        "\n",
        "check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    resource_id=resource_id,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        ")\n",
        "\n",
        "\n",
        "# @markdown A context length of 4096 is supported in the following configuration. The configuration has been validated for stability and performance.\n",
        "# @markdown The maximum context length is 32768.\n",
        "gpu_memory_utilization = 0.8\n",
        "context_length = 4096\n",
        "max_num_seqs = 128\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_sglang_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    context_length: int = 4096,\n",
        "    dtype: str = \"\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_flashinfer_mla: bool = False,\n",
        "    disable_cuda_graph: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    is_spot: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.sglang.ai/backend/server_arguments.html for a list of possible arguments with descriptions.\n",
        "    sglang_args = [\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tp={accelerator_count * multihost_gpu_node_count}\",\n",
        "        f\"--context-length={context_length}\",\n",
        "        f\"--max-running-requests={max_num_seqs}\",\n",
        "        f\"--mem-fraction-static={gpu_memory_utilization}\",\n",
        "    ]\n",
        "\n",
        "    if dtype:\n",
        "        sglang_args.append(f\"--dtype={dtype}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        sglang_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enable_flashinfer_mla:\n",
        "        sglang_args.append(\"--enable-flashinfer-mla\")\n",
        "\n",
        "    if disable_cuda_graph:\n",
        "        sglang_args.append(\"--disable-cuda-graph\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SGLANG_DOCKER_URI,\n",
        "        serving_container_args=sglang_args,\n",
        "        serving_container_ports=[30000],\n",
        "        serving_container_predict_route=\"/vertex_generate\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"serviceAccount\": service_account,\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_deepseek_deployment.ipynb\",\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"sglang_gpu\"], endpoints[\"sglang_gpu\"] = deploy_model_sglang_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"deepseek-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"deepseek-ai\",\n",
        "    publisher_model_id=(\"deepseek-v3\" if \"V3\" in model_id else \"deepseek-r1\"),\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    context_length=context_length,\n",
        "    enable_trust_remote_code=True,\n",
        "    enable_flashinfer_mla=True,\n",
        "    disable_cuda_graph=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    max_num_seqs=max_num_seqs,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AGVPzwHkn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by SGLang can be found [here](https://docs.sglang.ai/backend/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, so I need to figure out how to diagnose and fix a flickering light in my house. Hmm, where do I start? Let's think. First, I remember that flickering lights can be caused by various issues. Maybe the bulb is loose? That's a common problem. Let me start with the simplest things first.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-V3 model is shown below as an example.\n",
        "prompt = \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜>\"  # @param {type: \"string\"}\n",
        "# @markdown For the DeepSeek-R1 model, `<think>` should be appended to the prompt, as shown below.\n",
        "if model_id.lower().endswith(\"deepseek-r1\"):\n",
        "    prompt += \"<think>\\n\"\n",
        "\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_new_tokens`.\n",
        "max_new_tokens = 1024  # @param {type:\"integer\"}\n",
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "top_p = 0.95  # @param {type:\"number\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"sampling_params\": {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "    }\n",
        "}\n",
        "response = endpoints[\"sglang_gpu\"].predict(\n",
        "    instances=instances,\n",
        "    parameters=parameters,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_deepseek_deployment.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
